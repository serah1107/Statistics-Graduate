{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN LSTM을 이용한 DJI 주가 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rvVYU3r-GQc_"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hVX_o9EsNP1H"
   },
   "outputs": [],
   "source": [
    "def min_max_scaling(x):\n",
    "    x_np = np.asarray(x)\n",
    "    return (x_np - x_np.min()) / (x_np.max() - x_np.min() + 1e-8) # --- 0으로 나누는 오류를 예방하기 위함\n",
    "\n",
    "# 데이터 내의 너무 작거나 너무 큰 값이 학습을 방해하는 것을 방지하고자 \n",
    "# x(주가)를 최소값과 최대값을 이용하여 0~1 사이의 값으로 변환시켜 분석에 사용한다\n",
    "\n",
    "def reverse_min_max_scaling(org_x, x):\n",
    "    org_x_np = np.asarray(org_x)\n",
    "    x_np = np.asarray(x)\n",
    "    return (x_np * (org_x_np.max() - org_x_np.min() + 1e-7)) + org_x_np.min()\n",
    "\n",
    "# scale된 값을 다시 원래의 값으로 되돌리기 위한 함수\n",
    "# scaling하기 이전의 orginal x값과 되돌리고 싶은 x를 입력하면 다시 unscaling된 값으로 리턴한다\n",
    "# 학습에서 추정 이후 원래의 주가로 다시 변환하여 확인하기 위한 용도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0-2. 하이퍼파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d2fudzS-tD9z"
   },
   "outputs": [],
   "source": [
    "n_in = 6                   # input의 column 개수 (Feature(또는 Variable) 개수)\n",
    "n_out = 1                  # output의 column 개수 (하나의 주가 출력)\n",
    " \n",
    "seq_length = 28            # 1개 시퀀스의 길이 (시계열 데이터 입력 개수)\n",
    "n_hidden = 20              # 각 셀의 (hidden) 출력 크기\n",
    "forget_bias = 1.0          # 망각편향 (기본값 1.0)\n",
    " \n",
    "epochs = 500               # 총 학습 반복횟수\n",
    "learning_rate = 0.01       # optimizer의 학습률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JedZ-YvnQsY_"
   },
   "outputs": [],
   "source": [
    "# 텐서플로우 플레이스홀더 생성\n",
    "# 입력 X, 출력 Y를 생성한다\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, n_in]) # [None, 28, 6]                                                           \n",
    "Y = tf.placeholder(tf.float32, [None, n_out])                                        \n",
    "n_batch = tf.placeholder(tf.int32, [])\n",
    "\n",
    "# 검증용 측정지표(RMSE)를 산출하기 위해 targets, predictions를 생성한다\n",
    "# batch size에 상관없이 유동적으로 계산할 수 있음\n",
    "# targets에는 y의 실제 값, predictions에는 학습을 통해 예측된 값을 feed\n",
    "# RMSE = sqrt (targets - predictions)^2\n",
    "\n",
    "targets = tf.placeholder(tf.float32, [None, n_out])\n",
    "predictions = tf.placeholder(tf.float32, [None, n_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pHKZCNNWdo4h"
   },
   "outputs": [],
   "source": [
    "# LSTM 네트워크 생성\n",
    "def inference():\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units = n_hidden,       # cell의 차원은 [n_batch, n_hidden]\n",
    "                                        forget_bias = forget_bias,  # (망각편향)\n",
    "                                        state_is_tuple = True,      # c-state와 a-state를 tuple의 형태로 출력\n",
    "                                        activation = tf.tanh)  \n",
    "\n",
    "    init_state = cell.zero_state(n_batch, tf.float32)               # cell의 모든 값을 0으로 초기화\n",
    "    state = init_state\n",
    "\n",
    "    outputs = []\n",
    "    with tf.variable_scope(\"LSTM\"):                                 \n",
    "        for t in range(seq_length):\n",
    "            if t > 0:\n",
    "                tf.get_variable_scope().reuse_variables()           # with문 안에서 이미 존재하는 변수들을 다시 가져와 사용\n",
    "\n",
    "            (cell_output, state) = cell(X[:, t, :], state)          # t마다 cell_output = y_hat(t), state = a(t)를 \n",
    "            outputs.append(cell_output)                             # 다음 시점으로 output\n",
    "\n",
    "        output = outputs[-1]                                        # 1,...,28시점의 x로 추정한 29번째 y_hat\n",
    "\n",
    "        W = tf.Variable(tf.truncated_normal([n_hidden, n_out], stddev = 0.01))\n",
    "        b = tf.Variable(tf.zeros([n_out]))\n",
    "\n",
    "        y = tf.matmul(output, W) + b                                # 최종 linear y_hat\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9119,
     "status": "ok",
     "timestamp": 1543566148982,
     "user": {
      "displayName": "이하경",
      "photoUrl": "",
      "userId": "05802400435936053791"
     },
     "user_tz": -540
    },
    "id": "oCxsEylVdZN6",
    "outputId": "b79c2f7e-1f16-412b-c39b-b78e3437ac78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-c11ab9624e09>:6: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n"
     ]
    }
   ],
   "source": [
    "hypothesis = inference()\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(hypothesis - Y))                    # 손실함수로 평균제곱오차(MSE)를 사용한다\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)                   # 최적화함수로 AdamOptimizer를 사용한다\n",
    "train_step = optimizer.minimize(loss)\n",
    " \n",
    "# RMSE (Root Mean Square Error)\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))    # 즉 평균 절댓값오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 학습 ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175049
    },
    "colab_type": "code",
    "id": "RtIgrXn-oiCL",
    "outputId": "2550d15f-ce68-432e-c943-81fbd8b81bdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 , current loss: 0.17657322\n",
      "epoch: 0 , current loss: 0.13028906\n",
      "epoch: 0 , current loss: 0.054683227\n",
      "epoch: 0 , current loss: 0.036317248\n",
      "epoch: 0 , current loss: 0.028465992\n",
      "epoch: 0 , current loss: 0.04917818\n",
      "epoch: 0 , current loss: 0.052669574\n",
      "epoch: 0 , current loss: 0.04686624\n",
      "epoch: 0 , current loss: 0.03778369\n",
      "epoch: 10 , current loss: 0.00015523151\n",
      "epoch: 10 , current loss: 0.00016744998\n",
      "epoch: 10 , current loss: 0.0002539865\n",
      "epoch: 10 , current loss: 0.00030275516\n",
      "epoch: 10 , current loss: 0.0003514488\n",
      "epoch: 10 , current loss: 0.00028088025\n",
      "epoch: 10 , current loss: 0.00013199747\n",
      "epoch: 10 , current loss: 0.00016680128\n",
      "epoch: 10 , current loss: 0.00023062793\n",
      "epoch: 20 , current loss: 0.00016158205\n",
      "epoch: 20 , current loss: 0.0002111062\n",
      "epoch: 20 , current loss: 0.00019038403\n",
      "epoch: 20 , current loss: 0.0002563028\n",
      "epoch: 20 , current loss: 0.00025368936\n",
      "epoch: 20 , current loss: 0.00026141808\n",
      "epoch: 20 , current loss: 0.00017477035\n",
      "epoch: 20 , current loss: 0.00017469813\n",
      "epoch: 20 , current loss: 0.00018638461\n",
      "epoch: 30 , current loss: 0.00022259036\n",
      "epoch: 30 , current loss: 0.00026589067\n",
      "epoch: 30 , current loss: 0.000120060526\n",
      "epoch: 30 , current loss: 0.00014363615\n",
      "epoch: 30 , current loss: 0.00016306764\n",
      "epoch: 30 , current loss: 0.0001912014\n",
      "epoch: 30 , current loss: 0.00017313939\n",
      "epoch: 30 , current loss: 0.00015298613\n",
      "epoch: 30 , current loss: 0.0001985607\n",
      "epoch: 40 , current loss: 0.000107891065\n",
      "epoch: 40 , current loss: 0.00016626906\n",
      "epoch: 40 , current loss: 0.00015862324\n",
      "epoch: 40 , current loss: 0.0002310541\n",
      "epoch: 40 , current loss: 0.00016162267\n",
      "epoch: 40 , current loss: 0.00021988565\n",
      "epoch: 40 , current loss: 0.000121914876\n",
      "epoch: 40 , current loss: 0.00030438264\n",
      "epoch: 40 , current loss: 0.00014641894\n",
      "epoch: 50 , current loss: 0.00019843286\n",
      "epoch: 50 , current loss: 0.00018776409\n",
      "epoch: 50 , current loss: 0.00018945671\n",
      "epoch: 50 , current loss: 0.00022545166\n",
      "epoch: 50 , current loss: 0.00017471191\n",
      "epoch: 50 , current loss: 0.00013532909\n",
      "epoch: 50 , current loss: 0.00016216992\n",
      "epoch: 50 , current loss: 0.0001593708\n",
      "epoch: 50 , current loss: 0.00015696659\n",
      "epoch: 60 , current loss: 0.00013559607\n",
      "epoch: 60 , current loss: 0.00015326115\n",
      "epoch: 60 , current loss: 0.00019957246\n",
      "epoch: 60 , current loss: 0.00013545144\n",
      "epoch: 60 , current loss: 0.00020472672\n",
      "epoch: 60 , current loss: 0.00018820858\n",
      "epoch: 60 , current loss: 0.00014196303\n",
      "epoch: 60 , current loss: 0.00014328935\n",
      "epoch: 60 , current loss: 0.00018233451\n",
      "epoch: 70 , current loss: 0.00014669004\n",
      "epoch: 70 , current loss: 0.0001982205\n",
      "epoch: 70 , current loss: 0.00013951551\n",
      "epoch: 70 , current loss: 0.00018923102\n",
      "epoch: 70 , current loss: 0.00014513306\n",
      "epoch: 70 , current loss: 0.00014137432\n",
      "epoch: 70 , current loss: 0.00017355138\n",
      "epoch: 70 , current loss: 0.00012850783\n",
      "epoch: 70 , current loss: 0.00017202616\n",
      "epoch: 80 , current loss: 0.00013824354\n",
      "epoch: 80 , current loss: 0.00016837116\n",
      "epoch: 80 , current loss: 0.00015635187\n",
      "epoch: 80 , current loss: 0.0001230599\n",
      "epoch: 80 , current loss: 0.00017266761\n",
      "epoch: 80 , current loss: 0.00013980271\n",
      "epoch: 80 , current loss: 0.00015089377\n",
      "epoch: 80 , current loss: 0.000250865\n",
      "epoch: 80 , current loss: 0.00015464566\n",
      "epoch: 90 , current loss: 0.00014045104\n",
      "epoch: 90 , current loss: 0.00012052658\n",
      "epoch: 90 , current loss: 0.0001256082\n",
      "epoch: 90 , current loss: 9.808636e-05\n",
      "epoch: 90 , current loss: 0.00014826805\n",
      "epoch: 90 , current loss: 0.000110453206\n",
      "epoch: 90 , current loss: 0.00015622923\n",
      "epoch: 90 , current loss: 0.00019951812\n",
      "epoch: 90 , current loss: 0.0001924265\n",
      "epoch: 100 , current loss: 0.00012202154\n",
      "epoch: 100 , current loss: 0.00017199907\n",
      "epoch: 100 , current loss: 0.00012466277\n",
      "epoch: 100 , current loss: 0.000180847\n",
      "epoch: 100 , current loss: 0.00019332819\n",
      "epoch: 100 , current loss: 0.000110037254\n",
      "epoch: 100 , current loss: 0.00023457136\n",
      "epoch: 100 , current loss: 0.00010850774\n",
      "epoch: 100 , current loss: 0.00021442324\n",
      "epoch: 110 , current loss: 0.000101355676\n",
      "epoch: 110 , current loss: 0.00016182847\n",
      "epoch: 110 , current loss: 0.00022176496\n",
      "epoch: 110 , current loss: 0.0001824227\n",
      "epoch: 110 , current loss: 0.00013968836\n",
      "epoch: 110 , current loss: 0.00015784084\n",
      "epoch: 110 , current loss: 9.506574e-05\n",
      "epoch: 110 , current loss: 0.0001710213\n",
      "epoch: 110 , current loss: 0.000106597\n",
      "epoch: 120 , current loss: 0.00017646607\n",
      "epoch: 120 , current loss: 7.8149315e-05\n",
      "epoch: 120 , current loss: 0.00013892583\n",
      "epoch: 120 , current loss: 0.00010495512\n",
      "epoch: 120 , current loss: 0.00012967538\n",
      "epoch: 120 , current loss: 0.00011914809\n",
      "epoch: 120 , current loss: 0.00019214493\n",
      "epoch: 120 , current loss: 0.00010814538\n",
      "epoch: 120 , current loss: 0.00011961036\n",
      "epoch: 130 , current loss: 0.00014279237\n",
      "epoch: 130 , current loss: 0.00015748094\n",
      "epoch: 130 , current loss: 0.0001192492\n",
      "epoch: 130 , current loss: 0.00018668432\n",
      "epoch: 130 , current loss: 0.00019175166\n",
      "epoch: 130 , current loss: 0.00015683394\n",
      "epoch: 130 , current loss: 0.00013558228\n",
      "epoch: 130 , current loss: 0.00020203096\n",
      "epoch: 130 , current loss: 0.00010793862\n",
      "epoch: 140 , current loss: 0.00010879617\n",
      "epoch: 140 , current loss: 0.00012783667\n",
      "epoch: 140 , current loss: 0.00013339276\n",
      "epoch: 140 , current loss: 0.00013492123\n",
      "epoch: 140 , current loss: 0.000102460355\n",
      "epoch: 140 , current loss: 0.00011136059\n",
      "epoch: 140 , current loss: 0.00012651336\n",
      "epoch: 140 , current loss: 0.00011205407\n",
      "epoch: 140 , current loss: 0.00017407973\n",
      "epoch: 150 , current loss: 9.671372e-05\n",
      "epoch: 150 , current loss: 0.00014419356\n",
      "epoch: 150 , current loss: 0.00012319493\n",
      "epoch: 150 , current loss: 0.00011790528\n",
      "epoch: 150 , current loss: 0.000106598025\n",
      "epoch: 150 , current loss: 0.000112680355\n",
      "epoch: 150 , current loss: 0.00012276902\n",
      "epoch: 150 , current loss: 0.0001412161\n",
      "epoch: 150 , current loss: 0.000117117015\n",
      "epoch: 160 , current loss: 0.00014643808\n",
      "epoch: 160 , current loss: 0.00012688796\n",
      "epoch: 160 , current loss: 0.000112488175\n",
      "epoch: 160 , current loss: 0.00011050119\n",
      "epoch: 160 , current loss: 0.00014303178\n",
      "epoch: 160 , current loss: 0.0001174592\n",
      "epoch: 160 , current loss: 0.00012580234\n",
      "epoch: 160 , current loss: 0.000112958165\n",
      "epoch: 160 , current loss: 0.00012718732\n",
      "epoch: 170 , current loss: 0.00011448204\n",
      "epoch: 170 , current loss: 0.00011042298\n",
      "epoch: 170 , current loss: 0.000120850535\n",
      "epoch: 170 , current loss: 0.00014543452\n",
      "epoch: 170 , current loss: 0.00011444003\n",
      "epoch: 170 , current loss: 8.898256e-05\n",
      "epoch: 170 , current loss: 0.00014046956\n",
      "epoch: 170 , current loss: 0.00010506649\n",
      "epoch: 170 , current loss: 0.00012015242\n",
      "epoch: 180 , current loss: 9.639381e-05\n",
      "epoch: 180 , current loss: 0.00019119725\n",
      "epoch: 180 , current loss: 0.00011519626\n",
      "epoch: 180 , current loss: 0.00018509319\n",
      "epoch: 180 , current loss: 0.00016492984\n",
      "epoch: 180 , current loss: 0.00012097873\n",
      "epoch: 180 , current loss: 0.00019421334\n",
      "epoch: 180 , current loss: 0.00014736042\n",
      "epoch: 180 , current loss: 0.00016436362\n",
      "epoch: 190 , current loss: 0.00015585322\n",
      "epoch: 190 , current loss: 0.00013294056\n",
      "epoch: 190 , current loss: 0.00012032341\n",
      "epoch: 190 , current loss: 8.558238e-05\n",
      "epoch: 190 , current loss: 0.0001124003\n",
      "epoch: 190 , current loss: 0.0001188333\n",
      "epoch: 190 , current loss: 9.591003e-05\n",
      "epoch: 190 , current loss: 0.00010153115\n",
      "epoch: 190 , current loss: 0.00014488457\n",
      "epoch: 200 , current loss: 0.00014285475\n",
      "epoch: 200 , current loss: 0.00010160369\n",
      "epoch: 200 , current loss: 0.00012943358\n",
      "epoch: 200 , current loss: 9.08609e-05\n",
      "epoch: 200 , current loss: 0.0001427038\n",
      "epoch: 200 , current loss: 0.00011363611\n",
      "epoch: 200 , current loss: 0.00012601106\n",
      "epoch: 200 , current loss: 0.00013197665\n",
      "epoch: 200 , current loss: 9.0806854e-05\n",
      "epoch: 210 , current loss: 0.000118635784\n",
      "epoch: 210 , current loss: 0.0001340291\n",
      "epoch: 210 , current loss: 0.00013428742\n",
      "epoch: 210 , current loss: 0.00010624811\n",
      "epoch: 210 , current loss: 8.628117e-05\n",
      "epoch: 210 , current loss: 0.00014900869\n",
      "epoch: 210 , current loss: 0.00010337666\n",
      "epoch: 210 , current loss: 0.00013770413\n",
      "epoch: 210 , current loss: 0.00010750403\n",
      "epoch: 220 , current loss: 0.00012531987\n",
      "epoch: 220 , current loss: 0.000120247125\n",
      "epoch: 220 , current loss: 0.00016507236\n",
      "epoch: 220 , current loss: 8.901755e-05\n",
      "epoch: 220 , current loss: 9.868908e-05\n",
      "epoch: 220 , current loss: 0.00010907726\n",
      "epoch: 220 , current loss: 9.746634e-05\n",
      "epoch: 220 , current loss: 8.095913e-05\n",
      "epoch: 220 , current loss: 0.00012917673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 230 , current loss: 0.000114097864\n",
      "epoch: 230 , current loss: 9.8260876e-05\n",
      "epoch: 230 , current loss: 0.00011607456\n",
      "epoch: 230 , current loss: 0.00013007136\n",
      "epoch: 230 , current loss: 0.00015010472\n",
      "epoch: 230 , current loss: 0.000103388615\n",
      "epoch: 230 , current loss: 0.0001640852\n",
      "epoch: 230 , current loss: 0.00011440026\n",
      "epoch: 230 , current loss: 0.00022647377\n",
      "epoch: 240 , current loss: 0.00013872865\n",
      "epoch: 240 , current loss: 0.00013154146\n",
      "epoch: 240 , current loss: 0.00011101109\n",
      "epoch: 240 , current loss: 0.00011202443\n",
      "epoch: 240 , current loss: 0.000116901894\n",
      "epoch: 240 , current loss: 9.866886e-05\n",
      "epoch: 240 , current loss: 0.00014228122\n",
      "epoch: 240 , current loss: 0.0001052135\n",
      "epoch: 240 , current loss: 9.806568e-05\n",
      "epoch: 250 , current loss: 8.736602e-05\n",
      "epoch: 250 , current loss: 0.0001254366\n",
      "epoch: 250 , current loss: 0.000116269075\n",
      "epoch: 250 , current loss: 0.00011696518\n",
      "epoch: 250 , current loss: 0.00011322597\n",
      "epoch: 250 , current loss: 0.00010143798\n",
      "epoch: 250 , current loss: 0.00014052054\n",
      "epoch: 250 , current loss: 0.000112324095\n",
      "epoch: 250 , current loss: 0.00011100697\n",
      "epoch: 260 , current loss: 0.00014495358\n",
      "epoch: 260 , current loss: 0.00012145317\n",
      "epoch: 260 , current loss: 0.00013964821\n",
      "epoch: 260 , current loss: 0.00016745745\n",
      "epoch: 260 , current loss: 0.00012785412\n",
      "epoch: 260 , current loss: 0.000118118136\n",
      "epoch: 260 , current loss: 0.00011166839\n",
      "epoch: 260 , current loss: 0.000121148536\n",
      "epoch: 260 , current loss: 0.00011958884\n",
      "epoch: 270 , current loss: 8.806686e-05\n",
      "epoch: 270 , current loss: 0.00010071622\n",
      "epoch: 270 , current loss: 0.00011589247\n",
      "epoch: 270 , current loss: 0.000113915565\n",
      "epoch: 270 , current loss: 0.00012205441\n",
      "epoch: 270 , current loss: 0.00012924717\n",
      "epoch: 270 , current loss: 9.11885e-05\n",
      "epoch: 270 , current loss: 0.00013394971\n",
      "epoch: 270 , current loss: 9.900801e-05\n",
      "epoch: 280 , current loss: 8.660092e-05\n",
      "epoch: 280 , current loss: 0.00014256651\n",
      "epoch: 280 , current loss: 0.00015571472\n",
      "epoch: 280 , current loss: 7.410043e-05\n",
      "epoch: 280 , current loss: 0.00013330865\n",
      "epoch: 280 , current loss: 0.00010160592\n",
      "epoch: 280 , current loss: 0.00013532252\n",
      "epoch: 280 , current loss: 0.00010257591\n",
      "epoch: 280 , current loss: 0.00012573673\n",
      "epoch: 290 , current loss: 0.00010149995\n",
      "epoch: 290 , current loss: 9.3611525e-05\n",
      "epoch: 290 , current loss: 0.00016965937\n",
      "epoch: 290 , current loss: 0.000115394854\n",
      "epoch: 290 , current loss: 0.000117388256\n",
      "epoch: 290 , current loss: 0.00011983846\n",
      "epoch: 290 , current loss: 8.981887e-05\n",
      "epoch: 290 , current loss: 7.8547106e-05\n",
      "epoch: 290 , current loss: 0.00013276387\n",
      "epoch: 300 , current loss: 0.00016368063\n",
      "epoch: 300 , current loss: 0.00012365961\n",
      "epoch: 300 , current loss: 8.2810264e-05\n",
      "epoch: 300 , current loss: 0.000106195046\n",
      "epoch: 300 , current loss: 9.732111e-05\n",
      "epoch: 300 , current loss: 0.00010021789\n",
      "epoch: 300 , current loss: 0.0001131788\n",
      "epoch: 300 , current loss: 0.00012841116\n",
      "epoch: 300 , current loss: 0.00010167115\n",
      "epoch: 310 , current loss: 0.0001435395\n",
      "epoch: 310 , current loss: 0.00011639908\n",
      "epoch: 310 , current loss: 0.00016444278\n",
      "epoch: 310 , current loss: 0.000120562705\n",
      "epoch: 310 , current loss: 7.257658e-05\n",
      "epoch: 310 , current loss: 0.00011149421\n",
      "epoch: 310 , current loss: 0.000118717086\n",
      "epoch: 310 , current loss: 9.327065e-05\n",
      "epoch: 310 , current loss: 0.00011290184\n",
      "epoch: 320 , current loss: 0.000112981994\n",
      "epoch: 320 , current loss: 0.00016252112\n",
      "epoch: 320 , current loss: 8.661599e-05\n",
      "epoch: 320 , current loss: 9.361168e-05\n",
      "epoch: 320 , current loss: 9.816323e-05\n",
      "epoch: 320 , current loss: 0.00014568922\n",
      "epoch: 320 , current loss: 0.00011982317\n",
      "epoch: 320 , current loss: 9.177046e-05\n",
      "epoch: 320 , current loss: 8.510618e-05\n",
      "epoch: 330 , current loss: 0.00012987705\n",
      "epoch: 330 , current loss: 8.698578e-05\n",
      "epoch: 330 , current loss: 0.00015448357\n",
      "epoch: 330 , current loss: 0.0001109324\n",
      "epoch: 330 , current loss: 0.00014304336\n",
      "epoch: 330 , current loss: 0.00012221403\n",
      "epoch: 330 , current loss: 0.00015511051\n",
      "epoch: 330 , current loss: 9.604188e-05\n",
      "epoch: 330 , current loss: 0.00016559992\n",
      "epoch: 340 , current loss: 0.0001555081\n",
      "epoch: 340 , current loss: 0.00012297797\n",
      "epoch: 340 , current loss: 9.206876e-05\n",
      "epoch: 340 , current loss: 9.3579496e-05\n",
      "epoch: 340 , current loss: 9.988964e-05\n",
      "epoch: 340 , current loss: 0.00010487447\n",
      "epoch: 340 , current loss: 0.000120646844\n",
      "epoch: 340 , current loss: 0.00014808308\n",
      "epoch: 340 , current loss: 0.00012598185\n",
      "epoch: 350 , current loss: 0.00015794294\n",
      "epoch: 350 , current loss: 0.00012158107\n",
      "epoch: 350 , current loss: 0.00012455575\n",
      "epoch: 350 , current loss: 0.00011113976\n",
      "epoch: 350 , current loss: 0.00016725343\n",
      "epoch: 350 , current loss: 9.521069e-05\n",
      "epoch: 350 , current loss: 8.364161e-05\n",
      "epoch: 350 , current loss: 0.00013142789\n",
      "epoch: 350 , current loss: 0.00010611126\n",
      "epoch: 360 , current loss: 7.815793e-05\n",
      "epoch: 360 , current loss: 0.00020494906\n",
      "epoch: 360 , current loss: 9.897817e-05\n",
      "epoch: 360 , current loss: 0.00013623344\n",
      "epoch: 360 , current loss: 0.00011924868\n",
      "epoch: 360 , current loss: 0.00010469985\n",
      "epoch: 360 , current loss: 0.00012455187\n",
      "epoch: 360 , current loss: 9.510251e-05\n",
      "epoch: 360 , current loss: 0.00010464789\n",
      "epoch: 370 , current loss: 0.000112824775\n",
      "epoch: 370 , current loss: 8.290301e-05\n",
      "epoch: 370 , current loss: 0.00012612129\n",
      "epoch: 370 , current loss: 9.753685e-05\n",
      "epoch: 370 , current loss: 0.00010193148\n",
      "epoch: 370 , current loss: 0.00011337023\n",
      "epoch: 370 , current loss: 0.000121353434\n",
      "epoch: 370 , current loss: 0.00010998175\n",
      "epoch: 370 , current loss: 0.00015659232\n",
      "epoch: 380 , current loss: 0.00010389546\n",
      "epoch: 380 , current loss: 0.00014969578\n",
      "epoch: 380 , current loss: 9.173828e-05\n",
      "epoch: 380 , current loss: 0.0001256415\n",
      "epoch: 380 , current loss: 0.0001243046\n",
      "epoch: 380 , current loss: 0.0001145527\n",
      "epoch: 380 , current loss: 0.00011999174\n",
      "epoch: 380 , current loss: 0.000116499046\n",
      "epoch: 380 , current loss: 9.56457e-05\n",
      "epoch: 390 , current loss: 8.686382e-05\n",
      "epoch: 390 , current loss: 0.0002484273\n",
      "epoch: 390 , current loss: 0.000106044805\n",
      "epoch: 390 , current loss: 0.00024000881\n",
      "epoch: 390 , current loss: 0.00014700375\n",
      "epoch: 390 , current loss: 0.00011156377\n",
      "epoch: 390 , current loss: 0.00020056863\n",
      "epoch: 390 , current loss: 0.00012775137\n",
      "epoch: 390 , current loss: 0.00021165771\n",
      "epoch: 400 , current loss: 0.00016948328\n",
      "epoch: 400 , current loss: 9.864869e-05\n",
      "epoch: 400 , current loss: 0.00011545266\n",
      "epoch: 400 , current loss: 9.163187e-05\n",
      "epoch: 400 , current loss: 0.00010289913\n",
      "epoch: 400 , current loss: 0.000104468316\n",
      "epoch: 400 , current loss: 9.9767305e-05\n",
      "epoch: 400 , current loss: 0.00013694758\n",
      "epoch: 400 , current loss: 9.9207005e-05\n",
      "epoch: 410 , current loss: 0.000119525386\n",
      "epoch: 410 , current loss: 9.604853e-05\n",
      "epoch: 410 , current loss: 0.00012782679\n",
      "epoch: 410 , current loss: 0.0001498086\n",
      "epoch: 410 , current loss: 0.00012279084\n",
      "epoch: 410 , current loss: 0.00012291118\n",
      "epoch: 410 , current loss: 9.336856e-05\n",
      "epoch: 410 , current loss: 0.00012497706\n",
      "epoch: 410 , current loss: 9.667309e-05\n",
      "epoch: 420 , current loss: 0.00013082344\n",
      "epoch: 420 , current loss: 9.4875955e-05\n",
      "epoch: 420 , current loss: 0.00013367864\n",
      "epoch: 420 , current loss: 9.09173e-05\n",
      "epoch: 420 , current loss: 0.000119796015\n",
      "epoch: 420 , current loss: 0.0001651737\n",
      "epoch: 420 , current loss: 0.00011713414\n",
      "epoch: 420 , current loss: 0.000108940505\n",
      "epoch: 420 , current loss: 8.1273254e-05\n",
      "epoch: 430 , current loss: 0.00011212033\n",
      "epoch: 430 , current loss: 9.352391e-05\n",
      "epoch: 430 , current loss: 0.00013418122\n",
      "epoch: 430 , current loss: 0.00012415028\n",
      "epoch: 430 , current loss: 0.00015121023\n",
      "epoch: 430 , current loss: 9.876075e-05\n",
      "epoch: 430 , current loss: 0.00012112455\n",
      "epoch: 430 , current loss: 0.00015082008\n",
      "epoch: 430 , current loss: 0.00013353695\n",
      "epoch: 440 , current loss: 0.00012660616\n",
      "epoch: 440 , current loss: 0.00014295531\n",
      "epoch: 440 , current loss: 0.00012400775\n",
      "epoch: 440 , current loss: 8.3986095e-05\n",
      "epoch: 440 , current loss: 9.128416e-05\n",
      "epoch: 440 , current loss: 0.00011393038\n",
      "epoch: 440 , current loss: 0.00010002039\n",
      "epoch: 440 , current loss: 9.587356e-05\n",
      "epoch: 440 , current loss: 0.00012000763\n",
      "epoch: 450 , current loss: 0.00012024202\n",
      "epoch: 450 , current loss: 0.00016849433\n",
      "epoch: 450 , current loss: 0.000108643\n",
      "epoch: 450 , current loss: 9.872538e-05\n",
      "epoch: 450 , current loss: 0.00011684576\n",
      "epoch: 450 , current loss: 0.00010077209\n",
      "epoch: 450 , current loss: 0.00010088683\n",
      "epoch: 450 , current loss: 0.00015480026\n",
      "epoch: 450 , current loss: 0.00010894403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 460 , current loss: 7.989416e-05\n",
      "epoch: 460 , current loss: 0.0001213696\n",
      "epoch: 460 , current loss: 0.0001337402\n",
      "epoch: 460 , current loss: 0.000111066314\n",
      "epoch: 460 , current loss: 0.0001112907\n",
      "epoch: 460 , current loss: 0.00013456453\n",
      "epoch: 460 , current loss: 0.00010788062\n",
      "epoch: 460 , current loss: 0.00013436658\n",
      "epoch: 460 , current loss: 0.000100617275\n",
      "epoch: 470 , current loss: 8.920661e-05\n",
      "epoch: 470 , current loss: 0.00014170475\n",
      "epoch: 470 , current loss: 0.00010550507\n",
      "epoch: 470 , current loss: 0.00013425322\n",
      "epoch: 470 , current loss: 0.000121631194\n",
      "epoch: 470 , current loss: 0.00011797363\n",
      "epoch: 470 , current loss: 0.00014454575\n",
      "epoch: 470 , current loss: 9.971853e-05\n",
      "epoch: 470 , current loss: 0.00014314544\n",
      "epoch: 480 , current loss: 9.0432026e-05\n",
      "epoch: 480 , current loss: 0.00012425752\n",
      "epoch: 480 , current loss: 9.803431e-05\n",
      "epoch: 480 , current loss: 0.00010222645\n",
      "epoch: 480 , current loss: 0.0001356092\n",
      "epoch: 480 , current loss: 7.4552234e-05\n",
      "epoch: 480 , current loss: 8.652895e-05\n",
      "epoch: 480 , current loss: 0.00013924067\n",
      "epoch: 480 , current loss: 0.00016212175\n",
      "epoch: 490 , current loss: 0.00012154848\n",
      "epoch: 490 , current loss: 0.00013103841\n",
      "epoch: 490 , current loss: 0.0001142468\n",
      "epoch: 490 , current loss: 0.00012165794\n",
      "epoch: 490 , current loss: 0.00014795437\n",
      "epoch: 490 , current loss: 0.00012709762\n",
      "epoch: 490 , current loss: 0.00016217727\n",
      "epoch: 490 , current loss: 0.00014129076\n",
      "epoch: 490 , current loss: 0.00016141537\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "batch_size = 100\n",
    "n_batches = N_train // batch_size     # 학습 1회마다 9번의 mini-batch\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "my_loss = []\n",
    "for epoch in range(epochs):\n",
    "    X_, Y_ = shuffle(trainX, trainY)\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        sess.run(train_step, feed_dict = { X: X_[start:end], Y: Y_[start:end], n_batch: batch_size })\n",
    "        val_loss = sess.run(loss, feed_dict = { X: X_[start:end], Y: Y_[start:end], n_batch: batch_size })\n",
    "        my_loss.append(val_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(\"epoch:\" , epoch, \", current loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAHuhJREFUeJzt3X+QXWWd5/H3J935wQ8DCWkoSMgkSCwnijLaRFyVcXHAOKsEJUgYZkSHqqi71Ixr6RpmVmaWHWaHralxypEdxQEERPmpa6/EzbgiOrtKTIP8ClSgCZG0wUlDIIBA0p1894/nufZJc/ucm7590kn686q6dc95znOe8zxP33u+93nO6XsVEZiZmY3VlImugJmZHdgcSMzMrC0OJGZm1hYHEjMza4sDiZmZtcWBxMzM2uJAYmZmbXEgMTOztjiQmJlZWzonugL7wpw5c2LBggUTXQ0zswPKPffc83REdFXlmxSBZMGCBfT29k50NczMDiiSftFKPk9tmZlZWxxIzMysLQ4kZmbWFgcSMzNriwOJmZm1xYHEzMzaUmsgkbRU0gZJfZJWNdl+mqR7JQ1JWl5I/7eS7is8XpF0dt72NUlPFLadXGcbzMysXG3/RyKpA7gSOAPoB9ZJ6omIhwvZngQ+CnymuG9E/BA4OZczG+gD/rmQ5bMRcVtddW/4+tfhxRfhE5+o+0hmZgeuOkckS4C+iNgYETuBm4BlxQwRsSkiHgB2l5SzHPheRLxUX1Wb++Y34eqr9/VRzcwOLHUGkrnA5sJ6f07bWyuAb45Iu1zSA5K+IGn6WCvYiog6SzczO/DVGUjUJG2vTsuSjgVOAtYUki8BXg+cAswGPjfKvisl9UrqHRgY2JvDFsoY025mZpNKnYGkHzi+sD4P2LKXZXwY+HZEDDYSIuKpSHYA15Km0F4lIq6KiO6I6O7qqvzOMTMzG6M6A8k6YJGkhZKmkaaoevayjPMZMa2VRylIEnA28NA41HVUntoyMytXWyCJiCHgYtK01CPALRGxXtJlks4CkHSKpH7gXOArktY39pe0gDSi+dGIom+U9CDwIDAH+Ku62uCpLTOzarV+jXxErAZWj0i7tLC8jjTl1WzfTTS5OB8Rp49vLct5RGJmVs7/2V7CIxIzs2oOJGZm1hYHkgqe2jIzK+dAUsJTW2Zm1RxIKnhEYmZWzoGkhEckZmbVHEgqeERiZlbOgaSERyRmZtUcSMzMrC0OJBU8tWVmVs6BpISntszMqjmQVPCIxMysnANJCY9IzMyqOZCYmVlbHEgqeGrLzKycA0kJT22ZmVVzIKngEYmZWTkHkhIekZiZVXMgMTOzttQaSCQtlbRBUp+kVU22nybpXklDkpaP2LZL0n350VNIXyhpraTHJN0saVqdbfDUlplZudoCiaQO4ErgfcBi4HxJi0dkexL4KPCNJkW8HBEn58dZhfQrgC9ExCLgWeCica985qktM7NqdY5IlgB9EbExInYCNwHLihkiYlNEPADsbqVASQJOB27LSdcBZ49flV/NIxIzs3J1BpK5wObCen9Oa9UMSb2S7pbUCBZHAc9FxNAYy9wrHpGYmVXrrLHsZqfhvfl8Pz8itkg6AbhT0oPA862WKWklsBJg/vz5e3HYEYV7RGJmVqrOEUk/cHxhfR6wpdWdI2JLft4I3AX8DvA0cKSkRgActcyIuCoiuiOiu6ura+9rj0ckZmatqDOQrAMW5buspgErgJ6KfQCQNEvS9Lw8B3gH8HBEBPBDoHGH14XAd8a95mZm1rLaAkm+jnExsAZ4BLglItZLukzSWQCSTpHUD5wLfEXS+rz7bwO9ku4nBY6/iYiH87bPAZ+W1Ee6ZnJ1XW1I7aizdDOzA1+d10iIiNXA6hFplxaW15Gmp0bu9xPgpFHK3Ei6I6x2ntoyM6vm/2yv4BGJmVk5B5ISHpGYmVVzIDEzs7Y4kFTw1JaZWTkHkhKe2jIzq+ZAUsEjEjOzcg4kJTwiMTOr5kBSwSMSM7NyDiQlPCIxM6vmQGJmZm1xIKngqS0zs3IOJCU8tWVmVs2BpIJHJGZm5RxISnhEYmZWzYHEzMza4kBSwVNbZmblHEhKeGrLzKyaA0kFj0jMzMo5kJTwiMTMrFqtgUTSUkkbJPVJWtVk+2mS7pU0JGl5If1kST+VtF7SA5LOK2z7mqQnJN2XHyfX2QYzMyvXWVfBkjqAK4EzgH5gnaSeiHi4kO1J4KPAZ0bs/hLwkYh4TNJxwD2S1kTEc3n7ZyPitrrqXuSpLTOzcrUFEmAJ0BcRGwEk3QQsA34TSCJiU962u7hjRDxaWN4iaSvQBTzHPuSpLTOzanVObc0FNhfW+3PaXpG0BJgGPF5IvjxPeX1B0vT2qlnOIxIzs3J1BpJmn+f36rQs6VjgBuBjEdEYtVwCvB44BZgNfG6UfVdK6pXUOzAwsDeHLZQxpt3MzCaVOgNJP3B8YX0esKXVnSXNBO4A/nNE3N1Ij4inItkBXEuaQnuViLgqIrojorurq2tMDUjljHlXM7NJoc5Asg5YJGmhpGnACqCnlR1z/m8D10fErSO2HZufBZwNPDSutd7jWHWVbGZ28KgtkETEEHAxsAZ4BLglItZLukzSWQCSTpHUD5wLfEXS+rz7h4HTgI82uc33RkkPAg8Cc4C/qqsNZmZWrc67toiI1cDqEWmXFpbXkaa8Ru73deDro5R5+jhXs5SntszMyvk/20t4asvMrJoDSQWPSMzMyjmQlPCIxMysmgOJmZm1xYGkgqe2zMzKOZCU8NSWmVk1B5IKHpGYmZVzICnhEYmZWTUHkgoekZiZlXMgKeERiZlZNQcSMzNriwNJBU9tmZmVcyAp4aktM7NqDiQVPCIxMyvnQFLCIxIzs2oOJGZm1hYHkgqe2jIzK+dAUsJTW2Zm1RxIKnhEYmZWrtZAImmppA2S+iStarL9NEn3ShqStHzEtgslPZYfFxbS3yrpwVzmF6X6xg0ekZiZVastkEjqAK4E3gcsBs6XtHhEtieBjwLfGLHvbOAvgLcBS4C/kDQrb/5HYCWwKD+W1tQEMzNrQZ0jkiVAX0RsjIidwE3AsmKGiNgUEQ8Au0fs+17g+xGxLSKeBb4PLJV0LDAzIn4aEQFcD5xdYxs8tWVmVqHOQDIX2FxY789p7ew7Ny+Ppcy95qktM7NqdQaSZqfhVj/fj7Zvy2VKWimpV1LvwMBAi4dtUrhHJGZmpeoMJP3A8YX1ecCWNvftz8uVZUbEVRHRHRHdXV1dLVe6yCMSM7NqdQaSdcAiSQslTQNWAD0t7rsGOFPSrHyR/UxgTUQ8Bbwg6dR8t9ZHgO/UUfkGj0jMzMrVFkgiYgi4mBQUHgFuiYj1ki6TdBaApFMk9QPnAl+RtD7vuw34r6RgtA64LKcBfBL4J6APeBz4Xl1t8IjEzKxaZyuZJL0W6I+IHZLeDbwJuD4inivbLyJWA6tHpF1aWF7HnlNVxXzXANc0Se8F3thKvc3MrH6tjkhuB3ZJOhG4GljIiP/9OFh5asvMrFyrgWR3nqr6IPD3EfEfgWPrq9b+wVNbZmbVWg0kg5LOBy4EvpvTptZTpf2LRyRmZuVaDSQfA94OXB4RT0haCHy9vmrtHzwiMTOr1tLF9oh4GPgTgHw77msi4m/qrJiZmR0YWhqRSLpL0sz8ZYr3A9dK+rt6q7Z/8NSWmVm5Vqe2joiI54EPAddGxFuB36uvWvsHT22ZmVVrNZB05m/e/TDDF9snBY9IzMzKtRpILiP9h/rjEbFO0gnAY/VVa//gEYmZWbVWL7bfCtxaWN8InFNXpfYnHpGYmZVr9WL7PEnflrRV0r9Kul1S0682OZh4RGJmVq3Vqa1rSd/cexzph6T+V04zM7NJrtVA0hUR10bEUH58DRjbj3wcYDy1ZWZWrtVA8rSkP5TUkR9/CDxTZ8X2B57aMjOr1mog+WPSrb+/Ap4ClpO+NuWg5xGJmVm5lgJJRDwZEWdFRFdEHB0RZ5P+OfGg5hGJmVm1dn4h8dPjVgszMztgtRNIJsXndU9tmZmVayeQHPSnWE9tmZlVKw0kkl6Q9HyTxwuk/ykpJWmppA2S+iStarJ9uqSb8/a1khbk9Ask3Vd47JZ0ct52Vy6zse3oMbW8RR6RmJmVK/2KlIh4zVgLltQBXAmcAfQD6yT15N82abgIeDYiTpS0ArgCOC8ibgRuzOWcBHwnIu4r7HdBRPSOtW6tt6HuI5iZHfjamdqqsgToi4iNEbETuAlYNiLPMuC6vHwb8B7pVafv84Fv1ljPUh6RmJmVqzOQzAU2F9b7c1rTPBExBGwHjhqR5zxeHUiuzdNan28SeMaNRyRmZtXqDCTNTsMjP9+X5pH0NuCliHiosP2CiDgJeFd+/FHTg0srJfVK6h0YGNi7mpuZWcvqDCT9wPGF9XnAltHySOoEjgC2FbavYMRoJCJ+mZ9fAL5BmkJ7lYi4KiK6I6K7q2vsXwvmqS0zs3J1BpJ1wCJJCyVNIwWFnhF5eoAL8/Jy4M6IdOqWNAU4l3RthZzWKWlOXp4KvB94iJp4asvMrFpLP2w1FhExJOli0i8rdgDXRMR6SZcBvRHRA1wN3CCpjzQSWVEo4jSgP/+IVsN0YE0OIh3A/wG+WlcbUjvqLN3M7MBXWyABiIjVwOoRaZcWll8hjTqa7XsXcOqItF8Dbx33io7CIxIzs2p1Tm2Zmdkk4EBSwVNbZmblHEhKeGrLzKyaA0kFj0jMzMo5kJTwiMTMrJoDiZmZtcWBpIKntszMyjmQlPDUlplZNQcSMzNriwNJCY9IzMyqOZC0wNdJzMxG50BSwiMSM7NqDiRmZtYWB5IWeGrLzGx0DiQlPLVlZlbNgaQFHpGYmY3OgaSERyRmZtUcSMzMrC0OJC3w1JaZ2ehqDSSSlkraIKlP0qom26dLujlvXytpQU5fIOllSfflx5cL+7xV0oN5ny9K9U1ADQyk55dequsIZmYHvtoCiaQO4ErgfcBi4HxJi0dkuwh4NiJOBL4AXFHY9nhEnJwfnyik/yOwEliUH0vrasM//EN6vuGGuo5gZnbgq3NEsgToi4iNEbETuAlYNiLPMuC6vHwb8J6yEYakY4GZEfHTiAjgeuDs8a/6nnbvrvsIZmYHrjoDyVxgc2G9P6c1zRMRQ8B24Ki8baGkn0v6kaR3FfL3V5Q57nbtqvsIZmYHrs4ay242shh52Xq0PE8B8yPiGUlvBf6npDe0WGYqWFpJmgJj/vz5LVe6GY9IzMxGV+eIpB84vrA+D9gyWh5JncARwLaI2BERzwBExD3A48Drcv55FWWS97sqIrojorurq6uthviuLTOz0dUZSNYBiyQtlDQNWAH0jMjTA1yYl5cDd0ZESOrKF+uRdALpovrGiHgKeEHSqflaykeA79TYBsBTW2ZmZWqb2oqIIUkXA2uADuCaiFgv6TKgNyJ6gKuBGyT1AdtIwQbgNOAySUPALuATEbEtb/sk8DXgEOB7+VErT22ZmY2uzmskRMRqYPWItEsLy68A5zbZ73bg9lHK7AXeOL41LedAYmY2Ov9newscSMzMRudA0gIHEjOz0TmQtMCBxMxsdA4kJRr/Y+9AYmY2OgeSElNy7/j2XzOz0TmQlGgEEo9IzMxG50BSohFI/J/tZmajcyAp4aktM7NqDiQlOjrSs6e2zMxG50BSwtdIzMyqOZCUcCAxM6vmQFLC10jMzKo5kJTwiMTMrJoDSQnf/mtmVs2BpISntszMqjmQlPDUlplZNQeSEg4kZmbVHEhKNALJtddObD3MzPZnDiRmZtaWWgOJpKWSNkjqk7Sqyfbpkm7O29dKWpDTz5B0j6QH8/PphX3uymXelx9H11X/66+vq2Qzs4NHZ10FS+oArgTOAPqBdZJ6IuLhQraLgGcj4kRJK4ArgPOAp4EPRMQWSW8E1gBzC/tdEBG9ddW94Xd/t+4jmJkd+OockSwB+iJiY0TsBG4Clo3Iswy4Li/fBrxHkiLi5xGxJaevB2ZIml5jXc3MbIzqDCRzgc2F9X72HFXskScihoDtwFEj8pwD/DwidhTSrs3TWp+XGj+Ia2ZmE6HOQNLsBD/yf8RL80h6A2m66+OF7RdExEnAu/Ljj5oeXFopqVdS78DAwF5V3MzMWldnIOkHji+szwO2jJZHUidwBLAtr88Dvg18JCIeb+wQEb/Mzy8A3yBNob1KRFwVEd0R0d3V1TUuDTIzs1erM5CsAxZJWihpGrAC6BmRpwe4MC8vB+6MiJB0JHAHcElE/L9GZkmdkubk5anA+4GHamyDmZlVqC2Q5GseF5PuuHoEuCUi1ku6TNJZOdvVwFGS+oBPA41bhC8GTgQ+P+I23+nAGkkPAPcBvwS+WlcbzMysmmISfLVtd3d39PaO7W7hxqX8SdBNZmZ7kHRPRHRX5fN/tpuZWVscSMzMrC0OJGZm1hYHEjMza4sDiZmZtcWBpMIZZ6Rn37VlZtacA0mFTZvSc3//hFbDzGy/5UBS4dJL0/Ovfz2x9TAz2185kFSYNSs9P//8xNbDzGx/5UBSYcaM9PzKKxNbDzOz/ZUDSYXp+ee0duwoz2dmNlk5kFRwIDEzK+dAUsGBxMysnANJBQcSM7NyDiQVHEjMzMo5kFRwIDEzK+dAUsGBxMysnANJBQcSM7NyDiQVGv+Q+NJLE1sPM7P9Va2BRNJSSRsk9Ula1WT7dEk35+1rJS0obLskp2+Q9N5WyxxvHR3p+fLL6z6SmdmBqbZAIqkDuBJ4H7AYOF/S4hHZLgKejYgTgS8AV+R9FwMrgDcAS4H/IamjxTJrMTQEv/jFvjiSmdmBpbPGspcAfRGxEUDSTcAy4OFCnmXAX+bl24AvSVJOvykidgBPSOrL5dFCmbVZsACuvhre9jbo7ISZM2HnznQd5dBD4bDDYPfuFHSmTk3fGDx1KgwOwiGHDP+mycBA+jLIGTPS/p2dMGVK2v7cc2l59ux0XUZKZUQMlyul40xp8jGgkV58jhgeWVUZ+bsr0tj6qlHOWPcfq0afT5u2b49bFLHv270/29v+aKf/XnopvU+mT0+v/6lTx1bOWOxNvQ+210idgWQusLmw3g+8bbQ8ETEkaTtwVE6/e8S+c/NyVZnj7s/+DP76r9PyRRfVfbS9M3Pm8DcTH3bYnsGraOrU9Obq7Ewv4ldeGb6B4Mgj0/PQELz4YvNjTJmSXvhTpsAzz8Dhh++Z97DDUtqvfw27dsHLL7+6nCOOSNsgBbZGUJwyJe3X0QHbt6f6dHamINh4RAyfHKThN2FjWUpteuaZ4eMdfXRqk5TaOnVqeuzaNXyy2bEjHXdwEF54Ie3X1ZWO1zh2I5A3gvmhh6YPAB0dqT6N/Xft2vP4kD4QbNs2vD59empfo/yOjvR48cVXB9+R7Wz2/PLLw9fvjjhi+AQ1ZQo8+2z6sHL44ekYjb/31KmpHY2/5dSpKU9jG6RypkxJj507U9rgYArQ06alYwwOpnIj0nLEcFvnzBnul2L7jzoq9UHxbzs4mI7x8svpdRQxfIzGa6TRN4ODqV2Nvy+k10pHB2wunhkKZs5MZWzfnsp7zWvSB7vBwfSaef75VN9i3zY+pDXa1Wjnzp1p/6GhtG+jv55+Oj0femjq87K/XeOYs2cPvyYaf5+hodT2wcGUZ2gIjjkm9dPAQMrfqP/WrWl91qz02mucB2bNSuVt25aOcffd8NrXNu+b8VJnIGkWb0f+zuBoeUZLbzYV1/S3CyWtBFYCzJ8/f/RatuDyy+HjH4dPfQp+/ONXnywWLEgvsuOPh3e+E+64I73Yiz+GdfbZcP/98MQTw2nnnAO3356WOzrghBPgscfS+sUXw5e+lN4s55wDd94JGzakbZ/5DPzt38IHP5iOuXVrOokcdlh6Mx1zDNxyCyxbBj/5SXphveUtwyePjo705nvkkfSCnDs31f+ZZ2DjxvTCmz8fnnwSli6F171u+ES6e3dKnzVreAT16KPwjncMvxmmTk39dOqp8Pjj8KY3DZ8Ytm9P9T3hhOHyIuCXv0xvwNmzUz1nzBg+kU2Zkt4or7wy/AZv1KfxaKRdc01aPvnk1Obp09O+994LJ52U6hGR6jljRjp+Y99/+ZdUx3e/e/i4Ujp5TJmS3tyNk9q0aameEan8zs5Uv+uvT+WdeWbqj+OOS22bOzeNZj/wgdTGxkmlcRI+9NDhuhXb11ge7bkR/J98MvVp8ST26KPp+bjjUvmbNqXtxxwznOehh1LdZs8ebmNx9Ds0lF4jmzenfY88MrWzOIpuvJ6mTEkn1DvuSK/Zxon31lvTB4XXv354NF/820ak1/3WrfDmN6eA3tmZ3j8nnTT8/pBSfQYHYe3a9Jrr7Bz+YPDoo+n1XvSBD6Q6794NN94IK1akE3EjCDQC+BFHDPdp4zXZ+DDWeM80+qzxd58xYzhw33tvet/8wR+k11zZ32xwML0/3vWu1Lc7dqS0GTPS88svp8C+bVvql7e/PR3/W99K/TdzZvp7XnVVasN556VjbtwIa9bAhz6U0h97LAWQQw6hdoqafkNW0tuBv4yI9+b1SwAi4r8V8qzJeX4qqRP4FdAFrCrmbeTLu5WW2Ux3d3f09vaOX+PMzCYBSfdERHdVvjrv2loHLJK0UNI00sXznhF5eoAL8/Jy4M5Ika0HWJHv6loILAJ+1mKZZma2D9U2tZWveVwMrAE6gGsiYr2ky4DeiOgBrgZuyBfTt5ECAznfLaSL6EPAf4iIXQDNyqyrDWZmVq22qa39iae2zMz23v4wtWVmZpOAA4mZmbXFgcTMzNriQGJmZm1xIDEzs7ZMiru2JA0AY/3KxTnA0+NYnQOd+2OY+2JP7o89HQz98VsR0VWVaVIEknZI6m3l9rfJwv0xzH2xJ/fHniZTf3hqy8zM2uJAYmZmbXEgqXbVRFdgP+P+GOa+2JP7Y0+Tpj98jcTMzNriEYmZmbXFgaSEpKWSNkjqk7RqoutTB0nXSNoq6aFC2mxJ35f0WH6eldMl6Yu5Px6Q9JbCPhfm/I9JurDZsQ4Eko6X9ENJj0haL+lPc/qk6xNJMyT9TNL9uS/+S05fKGltbtfN+ScdyD/7cHPui7WSFhTKuiSnb5D03olp0fiQ1CHp55K+m9cndX8AEBF+NHmQvqb+ceAEYBpwP7B4outVQztPA94CPFRI++/Aqry8CrgiL/8+8D3SL1ieCqzN6bOBjfl5Vl6eNdFtG2N/HAu8JS+/BngUWDwZ+yS36fC8PBVYm9t4C7Aip38Z+GRe/vfAl/PyCuDmvLw4v3+mAwvz+6pjotvXRr98GvgG8N28Pqn7IyI8IimxBOiLiI0RsRO4CVg2wXUadxHxY9JvwRQtA67Ly9cBZxfSr4/kbuBISccC7wW+HxHbIuJZ4PvA0vprP/4i4qmIuDcvvwA8AsxlEvZJbtOLeXVqfgRwOnBbTh/ZF40+ug14jyTl9JsiYkdEPAH0kd5fBxxJ84B/B/xTXheTuD8aHEhGNxfYXFjvz2mTwTER8RSkEytwdE4frU8Oyr7KUxG/Q/okPin7JE/j3AdsJQXDx4HnImIoZym26zdtztu3A0dxkPRF9vfAfwJ25/WjmNz9ATiQlFGTtMl+i9tofXLQ9ZWkw4HbgU9FxPNlWZukHTR9EhG7IuJkYB7pU/NvN8uWnw/qvpD0fmBrRNxTTG6SdVL0R5EDyej6geML6/OALRNUl33tX/P0DPl5a04frU8Oqr6SNJUURG6MiG/l5EndJxHxHHAX6RrJkZIaP9NdbNdv2py3H0GaNj1Y+uIdwFmSNpGmuk8njVAma3/8hgPJ6NYBi/IdGdNIF8t6JrhO+0oP0LjL6ELgO4X0j+Q7lU4FtudpnjXAmZJm5buZzsxpB5w8h3018EhE/F1h06TrE0ldko7My4cAv0e6ZvRDYHnONrIvGn20HLgz0tXlHmBFvotpIbAI+Nm+acX4iYhLImJeRCwgnQ/ujIgLmKT9sYeJvtq/Pz9Id+Q8SpoX/vOJrk9Nbfwm8BQwSPqkdBFpHvcHwGP5eXbOK+DK3B8PAt2Fcv6YdNGwD/jYRLerjf54J2ma4QHgvvz4/cnYJ8CbgJ/nvngIuDSnn0A68fUBtwLTc/qMvN6Xt59QKOvPcx9tAN430W0bh755N8N3bU36/vB/tpuZWVs8tWVmZm1xIDEzs7Y4kJiZWVscSMzMrC0OJGZm1hYHErNxIGmXpPsKj3H7tmhJC1T4dmaz/U1ndRYza8HLkb5KxGzS8YjErEaSNkm6Iv+ux88knZjTf0vSD/JvmPxA0vycfoykb+ffALlf0r/JRXVI+mr+XZB/zv9pbrZfcCAxGx+HjJjaOq+w7fmIWAJ8ifTdTOTl6yPiTcCNwBdz+heBH0XEm0m/E7M+py8CroyINwDPAefU3B6zlvk/283GgaQXI+LwJumbgNMjYmP+MshfRcRRkp4Gjo2IwZz+VETMkTQAzIuIHYUyFpB+22RRXv8cMDUi/qr+lplV84jErH4xyvJoeZrZUVjeha9v2n7EgcSsfucVnn+al39C+gZZgAuA/5uXfwB8En7zo1Iz91UlzcbKn2rMxsch+ZcEG/53RDRuAZ4uaS3pg9v5Oe1PgGskfRYYAD6W0/8UuErSRaSRxydJ385stt/yNRKzGuVrJN0R8fRE18WsLp7aMjOztnhEYmZmbfGIxMzM2uJAYmZmbXEgMTOztjiQmJlZWxxIzMysLQ4kZmbWlv8Pqvv97HMyQQYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(my_loss, 'b')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 완료된 모형의 예측력 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. 학습 데이터와 테스트 데이터의 RMSE 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train과 test의 각각 예측된 y_hat\n",
    "train_predict = hypothesis.eval(session = sess, feed_dict = { X: trainX, n_batch: trainX.shape[0] })\n",
    "test_predict = hypothesis.eval(session = sess, feed_dict = { X: testX, n_batch: testX.shape[0] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.010313703 Test RMSE: 0.011190885\n"
     ]
    }
   ],
   "source": [
    "# train과 test의 RMSE\n",
    "train_rmse = rmse.eval(session = sess, feed_dict = { targets: trainY, predictions: train_predict })\n",
    "test_rmse = rmse.eval(session = sess, feed_dict = { targets: testY, predictions: test_predict })\n",
    "print(\"Train RMSE:\", train_rmse, \"Test RMSE:\", test_rmse)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM-DJI주가예측.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
