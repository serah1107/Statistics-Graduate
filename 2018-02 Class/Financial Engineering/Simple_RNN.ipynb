{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin(x, T=100):\n",
    "    return np.sin(2.0*np.pi*x/T)\n",
    "\n",
    "def toy_problem(T=100, ampl=0.05):\n",
    "    x = np.arange(0, 2*T + 1)\n",
    "    noise = ampl * np.random.uniform(low=-1.0, high=1.0, size=len(x))\n",
    "    return sin(x) + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=100\n",
    "f = toy_problem(T)\n",
    "x=np.arange(0, 2*T + 1)\n",
    "\n",
    "length_of_sequences = 2*T\n",
    "maxlen = 25\n",
    "\n",
    "data = []\n",
    "target = []\n",
    "\n",
    "for i in range(0, length_of_sequences - maxlen +1):\n",
    "    data.append(f[i:i+maxlen])\n",
    "    target.append(f[i+maxlen])\n",
    "    \n",
    "X = np.array(data).reshape(len(data), maxlen, 1)\n",
    "Y = np.array(target).reshape(len(data), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "N_train = int(len(data)*0.9)\n",
    "N_validation = len(data)-N_train\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=N_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = 1\n",
    "n_out = 1\n",
    "\n",
    "def inference(x,  n_batch, maxlen=None, n_hidden=None, n_out=None):\n",
    "    #V = tf.Variable(tf.truncated_normal([n_hidden, n_out], stddev=0.01))\n",
    "    #c = tf.Variable( tf.zeros([n_out], dtype=tf.float32) )\n",
    "    def weight_variable(shape):\n",
    "        return tf.Variable(tf.truncated_normal(shape, stddev=0.01))\n",
    "    def bias_variable(shape):\n",
    "        return tf.Variable(tf.zeros(shape))\n",
    "        \n",
    "    initial_state = tf.zeros(shape=(n_batch,n_hidden)) #need special case here!!!\n",
    "    state = initial_state\n",
    "    \n",
    "    outputs = []\n",
    "    Wxh = tf.Variable(tf.zeros([n_in,n_hidden]),name=\"Wxh\")\n",
    "    \n",
    "    Whh = tf.Variable(tf.zeros([n_hidden,n_hidden]),name=\"Whh\")\n",
    "    \n",
    "    bh = tf.Variable(tf.zeros([1,n_hidden]),name=\"bh\")\n",
    "    \n",
    "    \n",
    "    for t in range(maxlen):\n",
    "          \n",
    "            state = tf.tanh(tf.matmul(x[:,t,:],Wxh)+tf.matmul(state,Whh)+bh)\n",
    "            outputs.append(state)\n",
    "    output = outputs[-1]\n",
    "    \n",
    "    V = weight_variable([n_hidden, n_out])\n",
    "    c = bias_variable([n_out])\n",
    "    y = tf.matmul(output, V) + c\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden=2\n",
    "n_batch=5\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, maxlen, n_in])\n",
    "t = tf.placeholder(tf.float32, shape=[None, n_out])\n",
    "n_batch = tf.placeholder(tf.int32, shape=[])\n",
    "\n",
    "y = inference(x,  n_batch=n_batch, maxlen=maxlen, n_hidden=n_hidden, n_out=n_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, t):\n",
    "    mse = tf.reduce_mean(tf.square(y-t))\n",
    "    return mse\n",
    "\n",
    "\n",
    "def training(loss):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "    return train_step\n",
    "\n",
    "def accuracy(y,t):\n",
    "    correct_prediction= tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "    accuracy= tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "loss = loss(y,t)\n",
    "train_step = training(loss)\n",
    "accuracy=accuracy(y,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 validation loss: 0.51353073\n",
      "epoch: 1 validation loss: 0.5498291\n",
      "epoch: 2 validation loss: 0.5273528\n",
      "epoch: 3 validation loss: 0.38871843\n",
      "epoch: 4 validation loss: 0.5021299\n",
      "epoch: 5 validation loss: 0.52305305\n",
      "epoch: 6 validation loss: 0.6468361\n",
      "epoch: 7 validation loss: 0.6190767\n",
      "epoch: 8 validation loss: 0.49709716\n",
      "epoch: 9 validation loss: 0.4552853\n",
      "epoch: 10 validation loss: 0.50285584\n",
      "epoch: 11 validation loss: 0.3541779\n",
      "epoch: 12 validation loss: 0.36849958\n",
      "epoch: 13 validation loss: 0.58386695\n",
      "epoch: 14 validation loss: 0.6122253\n",
      "epoch: 15 validation loss: 0.5011386\n",
      "epoch: 16 validation loss: 0.6218786\n",
      "epoch: 17 validation loss: 0.45103684\n",
      "epoch: 18 validation loss: 0.543723\n",
      "epoch: 19 validation loss: 0.76318645\n",
      "epoch: 20 validation loss: 0.5511216\n",
      "epoch: 21 validation loss: 0.42640725\n",
      "epoch: 22 validation loss: 0.38453063\n",
      "epoch: 23 validation loss: 0.3437264\n",
      "epoch: 24 validation loss: 0.4658473\n",
      "epoch: 25 validation loss: 0.3959032\n",
      "epoch: 26 validation loss: 0.7450798\n",
      "epoch: 27 validation loss: 0.2817146\n",
      "epoch: 28 validation loss: 0.45002785\n",
      "epoch: 29 validation loss: 0.44118136\n",
      "epoch: 30 validation loss: 0.289878\n",
      "epoch: 31 validation loss: 0.42528105\n",
      "epoch: 32 validation loss: 0.5510089\n",
      "epoch: 33 validation loss: 0.52623034\n",
      "epoch: 34 validation loss: 0.46513158\n",
      "epoch: 35 validation loss: 0.5811519\n",
      "epoch: 36 validation loss: 0.48283917\n",
      "epoch: 37 validation loss: 0.45573464\n",
      "epoch: 38 validation loss: 0.4034133\n",
      "epoch: 39 validation loss: 0.5104344\n",
      "epoch: 40 validation loss: 0.47176924\n",
      "epoch: 41 validation loss: 0.40278912\n",
      "epoch: 42 validation loss: 0.51204646\n",
      "epoch: 43 validation loss: 0.675286\n",
      "epoch: 44 validation loss: 0.44288167\n",
      "epoch: 45 validation loss: 0.60216194\n",
      "epoch: 46 validation loss: 0.42530045\n",
      "epoch: 47 validation loss: 0.4795785\n",
      "epoch: 48 validation loss: 0.48807105\n",
      "epoch: 49 validation loss: 0.4712368\n",
      "epoch: 50 validation loss: 0.5043913\n",
      "epoch: 51 validation loss: 0.46743363\n",
      "epoch: 52 validation loss: 0.4976322\n",
      "epoch: 53 validation loss: 0.33744615\n",
      "epoch: 54 validation loss: 0.38854313\n",
      "epoch: 55 validation loss: 0.5389589\n",
      "epoch: 56 validation loss: 0.51375616\n",
      "epoch: 57 validation loss: 0.4492879\n",
      "epoch: 58 validation loss: 0.48691177\n",
      "epoch: 59 validation loss: 0.5553832\n",
      "epoch: 60 validation loss: 0.3942323\n",
      "epoch: 61 validation loss: 0.5538576\n",
      "epoch: 62 validation loss: 0.5323268\n",
      "epoch: 63 validation loss: 0.633862\n",
      "epoch: 64 validation loss: 0.47531986\n",
      "epoch: 65 validation loss: 0.59691894\n",
      "epoch: 66 validation loss: 0.37410504\n",
      "epoch: 67 validation loss: 0.5115215\n",
      "epoch: 68 validation loss: 0.5788709\n",
      "epoch: 69 validation loss: 0.5042097\n",
      "epoch: 70 validation loss: 0.6531582\n",
      "epoch: 71 validation loss: 0.2962588\n",
      "epoch: 72 validation loss: 0.6745106\n",
      "epoch: 73 validation loss: 0.5708579\n",
      "epoch: 74 validation loss: 0.6179035\n",
      "epoch: 75 validation loss: 0.48209706\n",
      "epoch: 76 validation loss: 0.41336203\n",
      "epoch: 77 validation loss: 0.4492887\n",
      "epoch: 78 validation loss: 0.42909127\n",
      "epoch: 79 validation loss: 0.5337819\n",
      "epoch: 80 validation loss: 0.4286936\n",
      "epoch: 81 validation loss: 0.49008536\n",
      "epoch: 82 validation loss: 0.7168681\n",
      "epoch: 83 validation loss: 0.49317303\n",
      "epoch: 84 validation loss: 0.403768\n",
      "epoch: 85 validation loss: 0.33429533\n",
      "epoch: 86 validation loss: 0.44370404\n",
      "epoch: 87 validation loss: 0.46935725\n",
      "epoch: 88 validation loss: 0.5050396\n",
      "epoch: 89 validation loss: 0.49425107\n",
      "epoch: 90 validation loss: 0.567202\n",
      "epoch: 91 validation loss: 0.30101275\n",
      "epoch: 92 validation loss: 0.20569317\n",
      "epoch: 93 validation loss: 0.43508047\n",
      "epoch: 94 validation loss: 0.3884049\n",
      "epoch: 95 validation loss: 0.4384871\n",
      "epoch: 96 validation loss: 0.43860793\n",
      "epoch: 97 validation loss: 0.52816135\n",
      "epoch: 98 validation loss: 0.47475234\n",
      "epoch: 99 validation loss: 0.5081809\n",
      "epoch: 100 validation loss: 0.50367373\n",
      "epoch: 101 validation loss: 0.46323767\n",
      "epoch: 102 validation loss: 0.42275438\n",
      "epoch: 103 validation loss: 0.23327446\n",
      "epoch: 104 validation loss: 0.515729\n",
      "epoch: 105 validation loss: 0.3988216\n",
      "epoch: 106 validation loss: 0.42712742\n",
      "epoch: 107 validation loss: 0.4957738\n",
      "epoch: 108 validation loss: 0.6838032\n",
      "epoch: 109 validation loss: 0.41793332\n",
      "epoch: 110 validation loss: 0.41580638\n",
      "epoch: 111 validation loss: 0.3654267\n",
      "epoch: 112 validation loss: 0.2415932\n",
      "epoch: 113 validation loss: 0.34827262\n",
      "epoch: 114 validation loss: 0.31958646\n",
      "epoch: 115 validation loss: 0.5770728\n",
      "epoch: 116 validation loss: 0.2759421\n",
      "epoch: 117 validation loss: 0.3684687\n",
      "epoch: 118 validation loss: 0.40474957\n",
      "epoch: 119 validation loss: 0.4122951\n",
      "epoch: 120 validation loss: 0.28237253\n",
      "epoch: 121 validation loss: 0.40247202\n",
      "epoch: 122 validation loss: 0.38906473\n",
      "epoch: 123 validation loss: 0.36996162\n",
      "epoch: 124 validation loss: 0.42023262\n",
      "epoch: 125 validation loss: 0.29481214\n",
      "epoch: 126 validation loss: 0.25677365\n",
      "epoch: 127 validation loss: 0.45639238\n",
      "epoch: 128 validation loss: 0.47451878\n",
      "epoch: 129 validation loss: 0.36697438\n",
      "epoch: 130 validation loss: 0.34102863\n",
      "epoch: 131 validation loss: 0.35260552\n",
      "epoch: 132 validation loss: 0.30056787\n",
      "epoch: 133 validation loss: 0.475173\n",
      "epoch: 134 validation loss: 0.3779295\n",
      "epoch: 135 validation loss: 0.37669778\n",
      "epoch: 136 validation loss: 0.453576\n",
      "epoch: 137 validation loss: 0.46664914\n",
      "epoch: 138 validation loss: 0.41728726\n",
      "epoch: 139 validation loss: 0.4387229\n",
      "epoch: 140 validation loss: 0.28719965\n",
      "epoch: 141 validation loss: 0.54196066\n",
      "epoch: 142 validation loss: 0.51625764\n",
      "epoch: 143 validation loss: 0.38914576\n",
      "epoch: 144 validation loss: 0.3492745\n",
      "epoch: 145 validation loss: 0.3868527\n",
      "epoch: 146 validation loss: 0.52237433\n",
      "epoch: 147 validation loss: 0.37055904\n",
      "epoch: 148 validation loss: 0.40330172\n",
      "epoch: 149 validation loss: 0.42772022\n",
      "epoch: 150 validation loss: 0.2769946\n",
      "epoch: 151 validation loss: 0.3301808\n",
      "epoch: 152 validation loss: 0.4265426\n",
      "epoch: 153 validation loss: 0.26610726\n",
      "epoch: 154 validation loss: 0.20932695\n",
      "epoch: 155 validation loss: 0.40550837\n",
      "epoch: 156 validation loss: 0.32880172\n",
      "epoch: 157 validation loss: 0.32985255\n",
      "epoch: 158 validation loss: 0.22342801\n",
      "epoch: 159 validation loss: 0.22712855\n",
      "epoch: 160 validation loss: 0.15668531\n",
      "epoch: 161 validation loss: 0.08961526\n",
      "epoch: 162 validation loss: 0.3325247\n",
      "epoch: 163 validation loss: 0.26531094\n",
      "epoch: 164 validation loss: 0.24933167\n",
      "epoch: 165 validation loss: 0.22818959\n",
      "epoch: 166 validation loss: 0.16786277\n",
      "epoch: 167 validation loss: 0.28614956\n",
      "epoch: 168 validation loss: 0.14080961\n",
      "epoch: 169 validation loss: 0.29886696\n",
      "epoch: 170 validation loss: 0.3854477\n",
      "epoch: 171 validation loss: 0.22381723\n",
      "epoch: 172 validation loss: 0.2975202\n",
      "epoch: 173 validation loss: 0.37461334\n",
      "epoch: 174 validation loss: 0.23212576\n",
      "epoch: 175 validation loss: 0.22197774\n",
      "epoch: 176 validation loss: 0.21319489\n",
      "epoch: 177 validation loss: 0.27546185\n",
      "epoch: 178 validation loss: 0.21861759\n",
      "epoch: 179 validation loss: 0.33884376\n",
      "epoch: 180 validation loss: 0.23580965\n",
      "epoch: 181 validation loss: 0.30913138\n",
      "epoch: 182 validation loss: 0.26302367\n",
      "epoch: 183 validation loss: 0.19774513\n",
      "epoch: 184 validation loss: 0.22313146\n",
      "epoch: 185 validation loss: 0.14457314\n",
      "epoch: 186 validation loss: 0.23851871\n",
      "epoch: 187 validation loss: 0.15919507\n",
      "epoch: 188 validation loss: 0.24439394\n",
      "epoch: 189 validation loss: 0.124117255\n",
      "epoch: 190 validation loss: 0.23351905\n",
      "epoch: 191 validation loss: 0.0994378\n",
      "epoch: 192 validation loss: 0.15605204\n",
      "epoch: 193 validation loss: 0.19779724\n",
      "epoch: 194 validation loss: 0.19343683\n",
      "epoch: 195 validation loss: 0.2051514\n",
      "epoch: 196 validation loss: 0.117729165\n",
      "epoch: 197 validation loss: 0.23000379\n",
      "epoch: 198 validation loss: 0.15579754\n",
      "epoch: 199 validation loss: 0.20493743\n",
      "epoch: 200 validation loss: 0.20189734\n",
      "epoch: 201 validation loss: 0.11615758\n",
      "epoch: 202 validation loss: 0.1160074\n",
      "epoch: 203 validation loss: 0.16869554\n",
      "epoch: 204 validation loss: 0.23726387\n",
      "epoch: 205 validation loss: 0.14074868\n",
      "epoch: 206 validation loss: 0.20682451\n",
      "epoch: 207 validation loss: 0.14956972\n",
      "epoch: 208 validation loss: 0.15273802\n",
      "epoch: 209 validation loss: 0.15008672\n",
      "epoch: 210 validation loss: 0.19578604\n",
      "epoch: 211 validation loss: 0.18186867\n",
      "epoch: 212 validation loss: 0.08029375\n",
      "epoch: 213 validation loss: 0.0954486\n",
      "epoch: 214 validation loss: 0.13436404\n",
      "epoch: 215 validation loss: 0.16539226\n",
      "epoch: 216 validation loss: 0.09887844\n",
      "epoch: 217 validation loss: 0.15276206\n",
      "epoch: 218 validation loss: 0.15673897\n",
      "epoch: 219 validation loss: 0.07948701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 220 validation loss: 0.18343243\n",
      "epoch: 221 validation loss: 0.12684597\n",
      "epoch: 222 validation loss: 0.1435954\n",
      "epoch: 223 validation loss: 0.14412366\n",
      "epoch: 224 validation loss: 0.13537475\n",
      "epoch: 225 validation loss: 0.10484479\n",
      "epoch: 226 validation loss: 0.08754555\n",
      "epoch: 227 validation loss: 0.07467885\n",
      "epoch: 228 validation loss: 0.08577422\n",
      "epoch: 229 validation loss: 0.082832195\n",
      "epoch: 230 validation loss: 0.1460472\n",
      "epoch: 231 validation loss: 0.12707095\n",
      "epoch: 232 validation loss: 0.13704014\n",
      "epoch: 233 validation loss: 0.110740446\n",
      "epoch: 234 validation loss: 0.117743716\n",
      "epoch: 235 validation loss: 0.1495612\n",
      "epoch: 236 validation loss: 0.08873714\n",
      "epoch: 237 validation loss: 0.12185036\n",
      "epoch: 238 validation loss: 0.102126196\n",
      "epoch: 239 validation loss: 0.09958787\n",
      "epoch: 240 validation loss: 0.1429955\n",
      "epoch: 241 validation loss: 0.14069043\n",
      "epoch: 242 validation loss: 0.11226032\n",
      "epoch: 243 validation loss: 0.12092185\n",
      "epoch: 244 validation loss: 0.09534134\n",
      "epoch: 245 validation loss: 0.09238456\n",
      "epoch: 246 validation loss: 0.06275323\n",
      "epoch: 247 validation loss: 0.097733855\n",
      "epoch: 248 validation loss: 0.11170568\n",
      "epoch: 249 validation loss: 0.04157582\n",
      "epoch: 250 validation loss: 0.06592746\n",
      "epoch: 251 validation loss: 0.085885145\n",
      "epoch: 252 validation loss: 0.071388565\n",
      "epoch: 253 validation loss: 0.09072909\n",
      "epoch: 254 validation loss: 0.038378254\n",
      "epoch: 255 validation loss: 0.106147625\n",
      "epoch: 256 validation loss: 0.08197782\n",
      "epoch: 257 validation loss: 0.09062351\n",
      "epoch: 258 validation loss: 0.11730446\n",
      "epoch: 259 validation loss: 0.07465055\n",
      "epoch: 260 validation loss: 0.06554826\n",
      "epoch: 261 validation loss: 0.045412026\n",
      "epoch: 262 validation loss: 0.07533945\n",
      "epoch: 263 validation loss: 0.036140274\n",
      "epoch: 264 validation loss: 0.093107305\n",
      "epoch: 265 validation loss: 0.07923928\n",
      "epoch: 266 validation loss: 0.038259614\n",
      "epoch: 267 validation loss: 0.07620236\n",
      "epoch: 268 validation loss: 0.105233766\n",
      "epoch: 269 validation loss: 0.075768545\n",
      "epoch: 270 validation loss: 0.10913138\n",
      "epoch: 271 validation loss: 0.09025038\n",
      "epoch: 272 validation loss: 0.065914564\n",
      "epoch: 273 validation loss: 0.0655697\n",
      "epoch: 274 validation loss: 0.07755\n",
      "epoch: 275 validation loss: 0.051708985\n",
      "epoch: 276 validation loss: 0.070685744\n",
      "epoch: 277 validation loss: 0.0982276\n",
      "epoch: 278 validation loss: 0.080310725\n",
      "epoch: 279 validation loss: 0.06252994\n",
      "epoch: 280 validation loss: 0.09943111\n",
      "epoch: 281 validation loss: 0.062344857\n",
      "epoch: 282 validation loss: 0.068191156\n",
      "epoch: 283 validation loss: 0.097529925\n",
      "epoch: 284 validation loss: 0.09174517\n",
      "epoch: 285 validation loss: 0.06937101\n",
      "epoch: 286 validation loss: 0.026289502\n",
      "epoch: 287 validation loss: 0.052601945\n",
      "epoch: 288 validation loss: 0.093691655\n",
      "epoch: 289 validation loss: 0.06841972\n",
      "epoch: 290 validation loss: 0.040687032\n",
      "epoch: 291 validation loss: 0.08165217\n",
      "epoch: 292 validation loss: 0.05796249\n",
      "epoch: 293 validation loss: 0.05533116\n",
      "epoch: 294 validation loss: 0.074507125\n",
      "epoch: 295 validation loss: 0.05087964\n",
      "epoch: 296 validation loss: 0.087424286\n",
      "epoch: 297 validation loss: 0.07506646\n",
      "epoch: 298 validation loss: 0.033731457\n",
      "epoch: 299 validation loss: 0.04702247\n",
      "epoch: 300 validation loss: 0.06402495\n",
      "epoch: 301 validation loss: 0.050717734\n",
      "epoch: 302 validation loss: 0.075863525\n",
      "epoch: 303 validation loss: 0.06397139\n",
      "epoch: 304 validation loss: 0.04856127\n",
      "epoch: 305 validation loss: 0.057076074\n",
      "epoch: 306 validation loss: 0.080743626\n",
      "epoch: 307 validation loss: 0.055042703\n",
      "epoch: 308 validation loss: 0.073433\n",
      "epoch: 309 validation loss: 0.036055792\n",
      "epoch: 310 validation loss: 0.04767535\n",
      "epoch: 311 validation loss: 0.04171138\n",
      "epoch: 312 validation loss: 0.06476744\n",
      "epoch: 313 validation loss: 0.05518382\n",
      "epoch: 314 validation loss: 0.06353389\n",
      "epoch: 315 validation loss: 0.035309874\n",
      "epoch: 316 validation loss: 0.06875198\n",
      "epoch: 317 validation loss: 0.0677645\n",
      "epoch: 318 validation loss: 0.026227647\n",
      "epoch: 319 validation loss: 0.04808841\n",
      "epoch: 320 validation loss: 0.06561193\n",
      "epoch: 321 validation loss: 0.059095044\n",
      "epoch: 322 validation loss: 0.05906849\n",
      "epoch: 323 validation loss: 0.043360166\n",
      "epoch: 324 validation loss: 0.059616543\n",
      "epoch: 325 validation loss: 0.074410066\n",
      "epoch: 326 validation loss: 0.041909836\n",
      "epoch: 327 validation loss: 0.050379075\n",
      "epoch: 328 validation loss: 0.053125907\n",
      "epoch: 329 validation loss: 0.04673366\n",
      "epoch: 330 validation loss: 0.059726648\n",
      "epoch: 331 validation loss: 0.02479129\n",
      "epoch: 332 validation loss: 0.0468568\n",
      "epoch: 333 validation loss: 0.055429827\n",
      "epoch: 334 validation loss: 0.056110263\n",
      "epoch: 335 validation loss: 0.055686712\n",
      "epoch: 336 validation loss: 0.049752634\n",
      "epoch: 337 validation loss: 0.076819345\n",
      "epoch: 338 validation loss: 0.05953855\n",
      "epoch: 339 validation loss: 0.03401682\n",
      "epoch: 340 validation loss: 0.057988115\n",
      "epoch: 341 validation loss: 0.04247398\n",
      "epoch: 342 validation loss: 0.039906714\n",
      "epoch: 343 validation loss: 0.06296395\n",
      "epoch: 344 validation loss: 0.03701862\n",
      "epoch: 345 validation loss: 0.06809772\n",
      "epoch: 346 validation loss: 0.040577598\n",
      "epoch: 347 validation loss: 0.058596797\n",
      "epoch: 348 validation loss: 0.048055198\n",
      "epoch: 349 validation loss: 0.05911944\n",
      "epoch: 350 validation loss: 0.039678365\n",
      "epoch: 351 validation loss: 0.053338457\n",
      "epoch: 352 validation loss: 0.060427427\n",
      "epoch: 353 validation loss: 0.04348197\n",
      "epoch: 354 validation loss: 0.046778213\n",
      "epoch: 355 validation loss: 0.06380001\n",
      "epoch: 356 validation loss: 0.03554063\n",
      "epoch: 357 validation loss: 0.05831451\n",
      "epoch: 358 validation loss: 0.056412507\n",
      "epoch: 359 validation loss: 0.076628976\n",
      "epoch: 360 validation loss: 0.036931194\n",
      "epoch: 361 validation loss: 0.04939223\n",
      "epoch: 362 validation loss: 0.039982103\n",
      "epoch: 363 validation loss: 0.034505185\n",
      "epoch: 364 validation loss: 0.06107363\n",
      "epoch: 365 validation loss: 0.046018798\n",
      "epoch: 366 validation loss: 0.03245484\n",
      "epoch: 367 validation loss: 0.03273306\n",
      "epoch: 368 validation loss: 0.04231565\n",
      "epoch: 369 validation loss: 0.054103702\n",
      "epoch: 370 validation loss: 0.041956\n",
      "epoch: 371 validation loss: 0.029691642\n",
      "epoch: 372 validation loss: 0.04470732\n",
      "epoch: 373 validation loss: 0.05402882\n",
      "epoch: 374 validation loss: 0.02898722\n",
      "epoch: 375 validation loss: 0.061496425\n",
      "epoch: 376 validation loss: 0.04347355\n",
      "epoch: 377 validation loss: 0.044964403\n",
      "epoch: 378 validation loss: 0.05597157\n",
      "epoch: 379 validation loss: 0.03189711\n",
      "epoch: 380 validation loss: 0.030570993\n",
      "epoch: 381 validation loss: 0.051083885\n",
      "epoch: 382 validation loss: 0.048593767\n",
      "epoch: 383 validation loss: 0.03421577\n",
      "epoch: 384 validation loss: 0.06199029\n",
      "epoch: 385 validation loss: 0.030217934\n",
      "epoch: 386 validation loss: 0.021021962\n",
      "epoch: 387 validation loss: 0.04307244\n",
      "epoch: 388 validation loss: 0.06675152\n",
      "epoch: 389 validation loss: 0.032392614\n",
      "epoch: 390 validation loss: 0.03552219\n",
      "epoch: 391 validation loss: 0.04512374\n",
      "epoch: 392 validation loss: 0.044874772\n",
      "epoch: 393 validation loss: 0.053287394\n",
      "epoch: 394 validation loss: 0.041336786\n",
      "epoch: 395 validation loss: 0.058606546\n",
      "epoch: 396 validation loss: 0.0397275\n",
      "epoch: 397 validation loss: 0.024026893\n",
      "epoch: 398 validation loss: 0.022926094\n",
      "epoch: 399 validation loss: 0.04364044\n",
      "epoch: 400 validation loss: 0.054650426\n",
      "epoch: 401 validation loss: 0.04538107\n",
      "epoch: 402 validation loss: 0.032914348\n",
      "epoch: 403 validation loss: 0.04469996\n",
      "epoch: 404 validation loss: 0.037336726\n",
      "epoch: 405 validation loss: 0.040103074\n",
      "epoch: 406 validation loss: 0.05369259\n",
      "epoch: 407 validation loss: 0.039520133\n",
      "epoch: 408 validation loss: 0.030050838\n",
      "epoch: 409 validation loss: 0.03126789\n",
      "epoch: 410 validation loss: 0.040613167\n",
      "epoch: 411 validation loss: 0.018338444\n",
      "epoch: 412 validation loss: 0.059697717\n",
      "epoch: 413 validation loss: 0.040703427\n",
      "epoch: 414 validation loss: 0.05208658\n",
      "epoch: 415 validation loss: 0.03192618\n",
      "epoch: 416 validation loss: 0.0629941\n",
      "epoch: 417 validation loss: 0.04398718\n",
      "epoch: 418 validation loss: 0.040818\n",
      "epoch: 419 validation loss: 0.030008767\n",
      "epoch: 420 validation loss: 0.03592642\n",
      "epoch: 421 validation loss: 0.02259786\n",
      "epoch: 422 validation loss: 0.026715105\n",
      "epoch: 423 validation loss: 0.024121378\n",
      "epoch: 424 validation loss: 0.042783268\n",
      "epoch: 425 validation loss: 0.039030157\n",
      "epoch: 426 validation loss: 0.018989217\n",
      "epoch: 427 validation loss: 0.0562054\n",
      "epoch: 428 validation loss: 0.021093687\n",
      "epoch: 429 validation loss: 0.059898622\n",
      "epoch: 430 validation loss: 0.040903397\n",
      "epoch: 431 validation loss: 0.036589842\n",
      "epoch: 432 validation loss: 0.024869554\n",
      "epoch: 433 validation loss: 0.04191211\n",
      "epoch: 434 validation loss: 0.048818037\n",
      "epoch: 435 validation loss: 0.04146485\n",
      "epoch: 436 validation loss: 0.021217974\n",
      "epoch: 437 validation loss: 0.040778276\n",
      "epoch: 438 validation loss: 0.020819632\n",
      "epoch: 439 validation loss: 0.038570512\n",
      "epoch: 440 validation loss: 0.038378615\n",
      "epoch: 441 validation loss: 0.04130746\n",
      "epoch: 442 validation loss: 0.053612906\n",
      "epoch: 443 validation loss: 0.026236141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 444 validation loss: 0.016720707\n",
      "epoch: 445 validation loss: 0.03603693\n",
      "epoch: 446 validation loss: 0.03496241\n",
      "epoch: 447 validation loss: 0.025793742\n",
      "epoch: 448 validation loss: 0.028926525\n",
      "epoch: 449 validation loss: 0.03546757\n",
      "epoch: 450 validation loss: 0.056509085\n",
      "epoch: 451 validation loss: 0.02803183\n",
      "epoch: 452 validation loss: 0.03506773\n",
      "epoch: 453 validation loss: 0.04312508\n",
      "epoch: 454 validation loss: 0.043214794\n",
      "epoch: 455 validation loss: 0.02692027\n",
      "epoch: 456 validation loss: 0.032143824\n",
      "epoch: 457 validation loss: 0.03035652\n",
      "epoch: 458 validation loss: 0.02246521\n",
      "epoch: 459 validation loss: 0.038854428\n",
      "epoch: 460 validation loss: 0.045458965\n",
      "epoch: 461 validation loss: 0.017931653\n",
      "epoch: 462 validation loss: 0.04998625\n",
      "epoch: 463 validation loss: 0.03901983\n",
      "epoch: 464 validation loss: 0.029976722\n",
      "epoch: 465 validation loss: 0.019552806\n",
      "epoch: 466 validation loss: 0.03658882\n",
      "epoch: 467 validation loss: 0.04889615\n",
      "epoch: 468 validation loss: 0.032774433\n",
      "epoch: 469 validation loss: 0.034837764\n",
      "epoch: 470 validation loss: 0.05790875\n",
      "epoch: 471 validation loss: 0.023989236\n",
      "epoch: 472 validation loss: 0.034421798\n",
      "epoch: 473 validation loss: 0.021498587\n",
      "epoch: 474 validation loss: 0.036178093\n",
      "epoch: 475 validation loss: 0.04571124\n",
      "epoch: 476 validation loss: 0.044531163\n",
      "epoch: 477 validation loss: 0.02791155\n",
      "epoch: 478 validation loss: 0.037068248\n",
      "epoch: 479 validation loss: 0.039650865\n",
      "epoch: 480 validation loss: 0.018946707\n",
      "epoch: 481 validation loss: 0.04153046\n",
      "epoch: 482 validation loss: 0.026531052\n",
      "epoch: 483 validation loss: 0.03487853\n",
      "epoch: 484 validation loss: 0.034557894\n",
      "epoch: 485 validation loss: 0.034386102\n",
      "epoch: 486 validation loss: 0.045513205\n",
      "epoch: 487 validation loss: 0.03611018\n",
      "epoch: 488 validation loss: 0.021693792\n",
      "epoch: 489 validation loss: 0.031136533\n",
      "epoch: 490 validation loss: 0.031600773\n",
      "epoch: 491 validation loss: 0.029991228\n",
      "epoch: 492 validation loss: 0.041009407\n",
      "epoch: 493 validation loss: 0.027443275\n",
      "epoch: 494 validation loss: 0.029940257\n",
      "epoch: 495 validation loss: 0.031593733\n",
      "epoch: 496 validation loss: 0.037309542\n",
      "epoch: 497 validation loss: 0.026931256\n",
      "epoch: 498 validation loss: 0.045266677\n",
      "epoch: 499 validation loss: 0.041885722\n",
      "Optimization Finished!\n",
      "Accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "batch_size = 10\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "n_batches = N_train // batch_size\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "my_loss = []\n",
    "acc=[]\n",
    "for epoch in range(epochs):\n",
    "    X_, Y_ = shuffle(X_train, Y_train)\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        start = i* batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "    sess.run(train_step, feed_dict={\n",
    "        x: X_[start:end], \n",
    "        t: Y_[start:end],\n",
    "        n_batch: batch_size\n",
    "    })\n",
    "    \n",
    "    val_loss = loss.eval(session=sess, feed_dict={\n",
    "        x: X_[start:end], \n",
    "        t: Y_[start:end],\n",
    "        n_batch: batch_size\n",
    "    })\n",
    "    my_loss.append(val_loss)\n",
    "    \n",
    "    print(\"epoch:\", epoch, \"validation loss:\", val_loss)\n",
    "    \n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "accuracy = accuracy.eval(session=sess, feed_dict={\n",
    "        x: X_validation, \n",
    "        t: Y_validation, \n",
    "        n_batch: N_validation\n",
    "    })\n",
    "print(\"Accuracy\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x216d4fb3c18>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(my_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import SimpleRNN\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def weight_variable(shape):\n",
    "        return tf.Variable(tf.truncated_normal(shape, stddev=0.01))\n",
    "    def bias_variable(shape):\n",
    "        return tf.Variable(tf.zeros(shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(n_hidden, \n",
    "                  init=weight_variable, \n",
    "                  input_shape=(maxlen, n_out)))\n",
    "model.add(Dense(n_out, init=weight_variable))\n",
    "model.add(Activation(\"linear\"))\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "batch_size = 10\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_validation, Y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate = maxlen\n",
    "Z=X[:1]\n",
    "original = [f[i] for i in range(maxlen)]\n",
    "predicted = [None for i in range(maxlen)]\n",
    "\n",
    "for i in range(length_of_sequences-maxlen+1):\n",
    "    z_ = Z[-1:]\n",
    "    y_ = model.predict(z_)\n",
    "    sequence_ = np.concatenate(\n",
    "       (z_.reshape(maxlen, n_in)[1:], y_), axis=0).reshape(1, maxlen, n_in)\n",
    "    Z=np.append(z_, sequence_, axis=0)\n",
    "    predicted.append(y_.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(toy_problem(T, ampl=0), linestyle=\"dotted\")\n",
    "plt.plot(original, linestyle=\"dashed\")\n",
    "plt.plot(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
