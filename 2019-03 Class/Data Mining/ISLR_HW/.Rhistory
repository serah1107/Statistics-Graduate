train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
Carseats.train <- Carseats[train, ]
Carseats.test <- Carseats[-train, ]
###b)------------------------------------------------------
library(tree)
tree.carseats <- tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
plot(tree.carseats);text(tree.carseats, pretty = 0)
yhat <- predict(tree.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
###c)------------------------------------------------------
cv.carseats <- cv.tree(tree.carseats)
plot(cv.carseats$size, cv.carseats$dev, type = "b")
tree.min <- which.min(cv.carseats$dev)
points(tree.min, cv.carseats$dev[tree.min], col = "red", cex = 2, pch = 20)
###d)------------------------------------------------------
prune.carseats <- prune.tree(tree.carseats, best = 8)
plot(prune.carseats);text(prune.carseats, pretty = 0)
yhat <- predict(prune.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
###d)------------------------------------------------------
bag.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 10, ntree = 500, importance = TRUE)
yhat.bag <- predict(bag.carseats, newdata = Carseats.test)
mean((yhat.bag - Carseats.test$Sales)^2)
importance(bag.carseats)
importance(bag.carseats);varImpPlot(rf.boston)
importance(bag.carseats);varImpPlot(bag.carseats)
# Chapter 8 Lab: Decision Trees
# Fitting Classification Trees
library(tree);library(ISLR);attach(Carseats)
#ex 8,9,10
##8----------------------------------------------------------------------
###a)------------------------------------------------------
library(ISLR)
set.seed(1)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
Carseats.train <- Carseats[train, ]
Carseats.train <- Carseats[train, ];Carseats.test <- Carseats[-train, ]
###b)------------------------------------------------------
library(tree)
tree.carseats <- tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
plot(tree.carseats);text(tree.carseats, pretty = 0)
yhat <- predict(tree.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
data("Carseats")
data(Carseats)
head(data(Carseats))
head(Carseats)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
Carseats.train <- Carseats[train, ];Carseats.test <- Carseats[-train, ]
###b)------------------------------------------------------
library(tree)
tree.carseats <- tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
plot(tree.carseats);text(tree.carseats, pretty = 0)
yhat <- predict(tree.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
###c)------------------------------------------------------
cv.carseats <- cv.tree(tree.carseats)
plot(cv.carseats$size, cv.carseats$dev, type = "b")
tree.min <- which.min(cv.carseats$dev)
points(tree.min, cv.carseats$dev[tree.min], col = "red", cex = 2, pch = 20)
prune.carseats <- prune.tree(tree.carseats, best = 8)
plot(prune.carseats);text(prune.carseats, pretty = 0)
yhat <- predict(prune.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
###b)------------------------------------------------------
library(tree)
tree.carseats <- tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
plot(tree.carseats);text(tree.carseats, pretty = 0)
yhat <- predict(tree.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
###c)------------------------------------------------------
cv.carseats <- cv.tree(tree.carseats)
plot(cv.carseats$size, cv.carseats$dev, type = "b")
tree.min <- which.min(cv.carseats$dev)
points(tree.min, cv.carseats$dev[tree.min], col = "red", cex = 2, pch = 20)
prune.carseats <- prune.tree(tree.carseats, best = 8)
###c)------------------------------------------------------
cv.carseats <- cv.tree(tree.carseats)
plot(cv.carseats$size, cv.carseats$dev, type = "b")
tree.min <- which.min(cv.carseats$dev)
points(tree.min, cv.carseats$dev[tree.min], col = "red", cex = 2, pch = 20)
prune.carseats <- prune.tree(tree.carseats, best = 8)
plot(prune.carseats);text(prune.carseats, pretty = 0)
yhat <- predict(prune.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
prune.carseats <- prune.tree(tree.carseats, best = 9)
plot(prune.carseats);text(prune.carseats, pretty = 0)
yhat <- predict(prune.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
set.seed(1)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
Carseats.train <- Carseats[train, ];Carseats.test <- Carseats[-train, ]
###b)------------------------------------------------------
library(tree)
tree.carseats <- tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
plot(tree.carseats);text(tree.carseats, pretty = 0)
yhat <- predict(tree.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
###c)------------------------------------------------------
cv.carseats <- cv.tree(tree.carseats)
plot(cv.carseats$size, cv.carseats$dev, type = "b")
tree.min <- which.min(cv.carseats$dev)
points(tree.min, cv.carseats$dev[tree.min], col = "red", cex = 2, pch = 20)
prune.carseats <- prune.tree(tree.carseats, best = 9)
plot(prune.carseats);text(prune.carseats, pretty = 0)
yhat <- predict(prune.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
###d)------------------------------------------------------
bag.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 10, ntree = 500, importance = TRUE)
yhat.bag <- predict(bag.carseats, newdata = Carseats.test)
mean((yhat.bag - Carseats.test$Sales)^2)
importance(bag.carseats);varImpPlot(bag.carseats)
###e)------------------------------------------------------
rf.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 3, ntree = 500, importance = TRUE)
yhat.rf <- predict(rf.carseats, newdata = Carseats.test)
mean((yhat.rf - Carseats.test$Sales)^2)
importance(rf.carseats)
importance(rf.carseats) ; varImpPlot(rf.carseats)
##9----------------------------------------------------------------------
###a)------------------------------------------------------
set.seed(1)
train <- sample(1:nrow(OJ), 800)
OJ.train <- OJ[train, ];OJ.test <- OJ[-train, ]
###b)------------------------------------------------------
tree.oj <- tree(Purchase ~ ., data = OJ.train);summary(tree.oj)
###b)------------------------------------------------------
tree.oj <- tree(Purchase ~ ., data = OJ.train);summary(tree.oj)
###c)------------------------------------------------------
tree.oj
###c)------------------------------------------------------
tree.oj ; varImpPlot(tree.oj)
###c)------------------------------------------------------
tree.oj ; importance(tree.oj)
###d)------------------------------------------------------
plot(tree.oj)
text(tree.oj, pretty = 0)
###e)------------------------------------------------------
tree.pred <- predict(tree.oj, OJ.test, type = "class")
table(tree.pred, OJ.test$Purchase)
table(tree.pred, OJ.test$Purchase) ; 1 - (147 + 62) / 270
tree.oj$frame
###e)------------------------------------------------------
tree.pred <- predict(tree.oj, OJ.test, type = "class")
table(tree.pred, OJ.test$Purchase) ; 1 - (147 + 62) / 270
###f)------------------------------------------------------
cv.oj <- cv.tree(tree.oj, FUN = prune.misclass);cv.oj
###g)------------------------------------------------------
plot(cv.oj$size, cv.oj$dev, type = "b", xlab = "Tree size", ylab = "Deviance")
###i)------------------------------------------------------
prune.oj <- prune.misclass(tree.oj, best = 2)
plot(prune.oj);text(prune.oj, pretty = 0)
###j)------------------------------------------------------
summary(tree.oj);summary(prune.oj)
###k)------------------------------------------------------
prune.pred <- predict(prune.oj, OJ.test, type = "class")
table(prune.pred, OJ.test$Purchase)
1 - (119 + 81) / 270
###k)------------------------------------------------------
prune.pred <- predict(prune.oj, OJ.test, type = "class")
table(prune.pred, OJ.test$Purchase)
1 - (119 + 81) / 270
##10----------------------------------------------------------------------
###a)------------------------------------------------------
Hitters <- na.omit(Hitters);Hitters$Salary <- log(Hitters$Salary)
###c)------------------------------------------------------
library(gbm);set.seed(1)
pows <- seq(-10, -0.2, by = 0.1);lambdas <- 10^pows
train.err <- rep(NA, length(lambdas))
for (i in 1:length(lambdas)) {
boost.hitters <- gbm(Salary ~ ., data = Hitters.train, distribution = "gaussian", n.trees = 1000, shrinkage = lambdas[i])
pred.train <- predict(boost.hitters, Hitters.train, n.trees = 1000)
train.err[i] <- mean((pred.train - Hitters.train$Salary)^2)
}
###b)------------------------------------------------------
train <- 1:200
Hitters.train <- Hitters[train, ];Hitters.test <- Hitters[-train, ]
###c)------------------------------------------------------
library(gbm);set.seed(1)
pows <- seq(-10, -0.2, by = 0.1);lambdas <- 10^pows
train.err <- rep(NA, length(lambdas))
for (i in 1:length(lambdas)) {
boost.hitters <- gbm(Salary ~ ., data = Hitters.train, distribution = "gaussian", n.trees = 1000, shrinkage = lambdas[i])
pred.train <- predict(boost.hitters, Hitters.train, n.trees = 1000)
train.err[i] <- mean((pred.train - Hitters.train$Salary)^2)
}
plot(lambdas, train.err, type = "b", xlab = "Shrinkage values", ylab = "Training MSE")
###d)------------------------------------------------------
set.seed(1)
test.err <- rep(NA, length(lambdas))
for (i in 1:length(lambdas)) {
boost.hitters <- gbm(Salary ~ ., data = Hitters.train, distribution = "gaussian", n.trees = 1000, shrinkage = lambdas[i])
yhat <- predict(boost.hitters, Hitters.test, n.trees = 1000)
test.err[i] <- mean((yhat - Hitters.test$Salary)^2)
}
plot(lambdas, test.err, type = "b", xlab = "Shrinkage values", ylab = "Test MSE")
min(test.err);lambdas[which.min(test.err)]
###e)------------------------------------------------------
library(glmnet)
fit1 <- lm(Salary ~ ., data = Hitters.train);pred1 <- predict(fit1, Hitters.test)
mean((pred1 - Hitters.test$Salary)^2)
###f)------------------------------------------------------
library(gbm)
boost.hitters <- gbm(Salary ~ ., data = Hitters.train, distribution = "gaussian", n.trees = 1000, shrinkage = lambdas[which.min(test.err)]);summary(boost.hitters)
###g)------------------------------------------------------
set.seed(1)
bag.hitters <- randomForest(Salary ~ ., data = hitters.train, mtry = 19, ntree = 500)
yhat.bag <- predict(bag.hitters, newdata = hitters.test)
bag.hitters <- randomForest(Salary ~ ., data = Hitters.train, mtry = 19, ntree = 500)
yhat.bag <- predict(bag.hitters, newdata = Hitters.test)
mean((yhat.bag - Hitters.test$Salary)^2)
# Chapter 9 Lab: Support Vector Machines
# Support Vector Classifier
set.seed(1)
x=matrix(rnorm(20*2), ncol=2); y=c(rep(-1,10), rep(1,10))
x[y==1,]=x[y==1,] + 1 ; plot(x, col=(3-y))
svmfit=svm(y~., data=dat, kernel="linear", cost=10,scale=FALSE)
plot(svmfit, dat)
library(e1071)
svmfit=svm(y~., data=dat, kernel="linear", cost=10,scale=FALSE)
dat=data.frame(x=x, y=as.factor(y))
library(e1071)
svmfit=svm(y~., data=dat, kernel="linear", cost=10,scale=FALSE)
svmfit=svm(y~., data=dat, kernel="linear", cost=10,scale=FALSE);plot(svmfit, dat)
svmfit$index;summary(svmfit)
svmfit=svm(y~., data=dat, kernel="linear", cost=0.1,scale=FALSE)
svmfit=svm(y~., data=dat, kernel="linear", cost=0.1,scale=FALSE);plot(svmfit, dat)
svmfit$index
svmfit=svm(y~., data=dat, kernel="linear", cost=10,scale=FALSE);plot(svmfit, dat)
svmfit=svm(y~., data=dat, kernel="linear", cost=0.1,scale=FALSE);plot(svmfit, dat)
tune.out=tune(svm,y~.,data=dat,kernel="linear",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.out)
xtest=matrix(rnorm(20*2), ncol=2)
xtest=matrix(rnorm(20*2), ncol=2);ytest=sample(c(-1,1), 20, rep=TRUE)
xtest[ytest==1,]=xtest[ytest==1,] + 1
testdat=data.frame(x=xtest, y=as.factor(ytest))
ypred=predict(bestmod,testdat);table(predict=ypred, truth=testdat$y)
bestmod=tune.out$best.model;summary(bestmod)
xtest=matrix(rnorm(20*2), ncol=2);ytest=sample(c(-1,1), 20, rep=TRUE)
xtest[ytest==1,]=xtest[ytest==1,] + 1
testdat=data.frame(x=xtest, y=as.factor(ytest))
ypred=predict(bestmod,testdat);table(predict=ypred, truth=testdat$y)
svmfit=svm(y~., data=dat, kernel="linear", cost=.01,scale=FALSE)
ypred=predict(svmfit,testdat);table(predict=ypred, truth=testdat$y)
x[y==1,]=x[y==1,]+0.5;plot(x, col=(y+5)/2, pch=19)
svmfit=svm(y~., data=dat, kernel="linear", cost=1e5)
summary(svmfit)
svmfit=svm(y~., data=dat, kernel="linear", cost=1e5);summary(svmfit);plot(svmfit, dat)
svmfit=svm(y~., data=dat, kernel="linear", cost=1);summary(svmfit);plot(svmfit,dat)
# Support Vector Machine
set.seed(1)
x=matrix(rnorm(200*2), ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
x=matrix(rnorm(200*2), ncol=2;x[1:100,]=x[1:100,]+2;x[101:150,]=x[101:150,]-2
train=sample(200,100)
x=matrix(rnorm(200*2), ncol=2 ;x[1:100,]=x[1:100,]+2
x=matrix(rnorm(200*2), ncol=2 );x[1:100,]=x[1:100,]+2;x[101:150,]=x[101:150,]-2;y=c(rep(1,150),rep(2,50));dat=data.frame(x=x,y=as.factor(y));plot(x, col=y)
train=sample(200,100);svmfit=svm(y~., data=dat[train,], kernel="radial",  gamma=1, cost=1)
plot(svmfit, dat[train,])
summary(svmfit)
svmfit=svm(y~., data=dat[train,], kernel="radial",gamma=1,cost=1e5)
tune.out=tune(svm, y~., data=dat[train,], kernel="radial", ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))
tune.out=tune(svm, y~., data=dat[train,], kernel="radial", ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)));summary(tune.out)
table(true=dat[-train,"y"], pred=predict(tune.out$best.model,newx=dat[-train,]))
par(mfrow=c(1,2))
rocplot(fitted,dat[train,"y"],main="Training Data")
svmfit.flex=svm(y~., data=dat[train,], kernel="radial",gamma=50, cost=1, decision.values=T)
# ROC Curves
library(ROCR)
rocplot=function(pred, truth, ...){
predob = prediction(pred, truth)
perf = performance(predob, "tpr", "fpr")
plot(perf,...)}
svmfit.opt=svm(y~., data=dat[train,], kernel="radial",gamma=2, cost=1,decision.values=T)
fitted=attributes(predict(svmfit.opt,dat[train,],decision.values=TRUE))$decision.values
par(mfrow=c(1,2))
rocplot(fitted,dat[train,"y"],main="Training Data")
svmfit.flex=svm(y~., data=dat[train,], kernel="radial",gamma=50, cost=1, decision.values=T)
fitted=attributes(predict(svmfit.flex,dat[train,],decision.values=T))$decision.values
rocplot(fitted,dat[train,"y"],add=T,col="red")
fitted=attributes(predict(svmfit.opt,dat[-train,],decision.values=T))$decision.values
rocplot(fitted,dat[-train,"y"],main="Test Data")
fitted=attributes(predict(svmfit.flex,dat[-train,],decision.values=T))$decision.values
rocplot(fitted,dat[-train,"y"],add=T,col="red")
# Chapter 9 Lab: Support Vector Machines
# Support Vector Classifier
set.seed(1)
x=matrix(rnorm(20*2), ncol=2); y=c(rep(-1,10), rep(1,10))
x[y==1,]=x[y==1,] + 1 ; plot(x, col=(3-y))
dat=data.frame(x=x, y=as.factor(y))
library(e1071)
par(mfrow = c(1,1))
# Chapter 9 Lab: Support Vector Machines
# Support Vector Classifier
set.seed(1)
x=matrix(rnorm(20*2), ncol=2); y=c(rep(-1,10), rep(1,10))
x[y==1,]=x[y==1,] + 1 ; plot(x, col=(3-y))
dat=data.frame(x=x, y=as.factor(y))
library(e1071)
svmfit=svm(y~., data=dat, kernel="linear", cost=10,scale=FALSE);plot(svmfit, dat)
svmfit$index;summary(svmfit)
svmfit=svm(y~., data=dat, kernel="linear", cost=0.1,scale=FALSE);plot(svmfit, dat)
tune.out=tune(svm,y~.,data=dat,kernel="linear",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)));summary(tune.out)
bestmod=tune.out$best.model;summary(bestmod)
xtest=matrix(rnorm(20*2), ncol=2);ytest=sample(c(-1,1), 20, rep=TRUE)
xtest[ytest==1,]=xtest[ytest==1,] + 1
x[y==1,]=x[y==1,]+0.5;plot(x, col=(y+5)/2, pch=19)
dat=data.frame(x=x,y=as.factor(y))
svmfit=svm(y~., data=dat, kernel="linear", cost=1e5);summary(svmfit);plot(svmfit, dat)
svmfit=svm(y~., data=dat, kernel="linear", cost=1);summary(svmfit);plot(svmfit,dat)
svmfit=svm(y~., data=dat, kernel="linear", cost=1e5);summary(svmfit);plot(svmfit, dat)
svmfit=svm(y~., data=dat, kernel="linear", cost=1e5);summary(svmfit);plot(svmfit, dat)
svmfit=svm(y~., data=dat, kernel="linear", cost=1);summary(svmfit);plot(svmfit,dat)
##ex 4,5,8
###ex4---------------------------------------------------------------------------
set.seed(1)
transl <- 3; X <- matrix(rnorm(100 * 2), ncol = 2)
X[1:30, ] <- X[1:30, ] + transl; X[31:60, ] <- X[31:60, ] - transl
y <- c(rep(0, 60), rep(1, 40));dat <- data.frame(x = X, y = as.factor(y))
plot(X, col = y + 1)
train <- sample(100, 80);dat.train <- dat[train, ];dat.test <- dat[-train, ]
###fit the svm model
library(e1071)
svm.lin <- svm(y ~ ., data = dat.train, kernel = 'linear', scale = FALSE);plot(svm.lin, data = dat.train)
svm.lin <- svm(y ~ ., data = dat.train, kernel = 'linear', scale = FALSE);summary(svm.lin)
table(predict = svm.lin$fitted, truth = dat.train$y)
####polynomial
svm.poly <- svm(y ~ ., data = dat.train, kernel = 'polynomial', scale = FALSE);plot(svm.poly, data = dat.train)
table(predict = svm.poly$fitted, truth = dat.train$y)
####radial
svm.rad <- svm(y ~ ., data = dat.train, kernel = 'radial', scale = FALSE);plot(svm.rad, data = dat.train)
lin.pred <- predict(svm.lin, dat.test);table(predict = lin.pred, truth = dat.test$y)
poly.pred <- predict(svm.poly, dat.test);table(predict = poly.pred, truth = dat.test$y)
rad.pred <- predict(svm.rad, dat.test);table(predict = rad.pred, truth = dat.test$y)
plot(svmfit, dat[train,])
##a)-----------------------------------------------------------------------------summary(svmfit)
svmfit=svm(y~., data=dat[train,], kernel="radial",gamma=1,cost=1e5);plot(svmfit,dat[train,])
##b)-----------------------------------------------------------------------------
plot(x1[y == 0], x2[y == 0], col = "red", xlab = "X1", ylab = "X2")
points(x1[y == 1], x2[y == 1], col = "blue")
x1 <- runif(500) - 0.5;x2 <- runif(500) - 0.5
y <- as.integer(x1 ^ 2 - x2 ^ 2 > 0)
##b)-----------------------------------------------------------------------------
plot(x1[y == 0], x2[y == 0], col = "red", xlab = "X1", ylab = "X2")
points(x1[y == 1], x2[y == 1], col = "blue")
##c)-----------------------------------------------------------------------------
dat <- data.frame(x1 = x1, x2 = x2, y = as.factor(y))
lr.fit <- glm(y ~ ., data = dat, family = 'binomial')
##d)-----------------------------------------------------------------------------
lr.prob <- predict(lr.fit, newdata = dat, type = 'response');lr.pred <- ifelse(lr.prob > 0.5, 1, 0)
plot(dat$x1, dat$x2, col = lr.pred + 2)
###ex5---------------------------------------------------------------------------
##a)-----------------------------------------------------------------------------
library(knitr);knitr::opts_chunk$set(echo = TRUE);library(tidyverse);library(ggthemes);library(caret);library(e1071)
theme_set(theme_tufte(base_size = 14))
df <- data.frame(replicate(2, rnorm(500)));df <- as.tibble(df)
class_func <- function(x, y) {
x^2 + y < 1
}
df <- df %>%
rename(Var1 = X1, Var2 = X2) %>%
mutate(Class = ifelse(class_func(Var1, Var2),
'Class A',
'Class B'),
Class = factor(Class))
##b)-----------------------------------------------------------------------------
inTrain <- sample(nrow(df), nrow(df)*0.6, replace = FALSE)
training <- df[inTrain,];testing <- df[-inTrain,]
ggplot(df, aes(Var1, Var2, col = Class)) +
geom_point(size = 2)
##c)-----------------------------------------------------------------------------
logreg_fit <- glm(Class ~ ., data = training, family = 'binomial');summary(logreg_fit)
##d)-----------------------------------------------------------------------------
pred <- predict(logreg_fit, testing, type = 'response');pred <- ifelse(pred >= 0.5, 'Class B', 'Class A')
mean(pred == testing$Class)
ggplot(df, aes(Var1, Var2, col = pred)) +
geom_point(size = 2)
ggplot(data.frame(df,pred), aes(Var1, Var2, col = pred)) +
geom_point(size = 2)
ggplot(data.frame(testing,pred), aes(Var1, Var2, col = pred)) +
geom_point(size = 2)
##e)-----------------------------------------------------------------------------
pred <- predict(logreg_fit, testing, type = 'response');pred <- ifelse(pred >= 0.5, 'Class B', 'Class A')
##f)-----------------------------------------------------------------------------
pred <- predict(logreg_fit, testing, type = 'response');pred <- ifelse(pred >= 0.5, 'Class B', 'Class A')
mean(pred == testing$Class)
ggplot(data.frame(testing,pred), aes(Var1, Var2, col = pred)) +
geom_point(size = 2)
##e)-----------------------------------------------------------------------------
logreg_fit <- glm(Class ~ poly(Var1, 2) + Var2, data = training, family = 'binomial');summary(logreg_fit)
##f)-----------------------------------------------------------------------------
pred <- predict(logreg_fit, testing, type = 'response');pred <- ifelse(pred >= 0.5, 'Class B', 'Class A')
mean(pred == testing$Class)
data_frame(pred = pred, class = testing$Class) %>%
ggplot(aes(pred, class)) +
geom_jitter()
data_frame(pred = pred, class = testing$Class) %>%
ggplot(aes(pred, class,col=pred)) +
geom_jitter()
data_frame(pred = pred, class = testing$Class) %>%
ggplot(aes(pred, class,col=class)) +
geom_jitter()
##g)-----------------------------------------------------------------------------
svm_fit <- svm(Class ~ ., data = training,kernel = 'linear',scale = FALSE);plot(svm_fit, testing)
mean(predict(svm_fit, testing) == testing$Class)
##h)-----------------------------------------------------------------------------
svm_poly <- svm(Class ~ ., data = training,kernel = 'polynomial', degree = 2,scale = FALSE);plot(svm_poly, testing)
mean(predict(svm_poly, testing) == testing$Class)
svm_fit <- svm(Class ~ ., data = training,kernel = 'radial',scale = FALSE);plot(svm_fit, testing)
mean(predict(svm_poly, testing) == testing$Class)
##h)-----------------------------------------------------------------------------
svm_poly <- svm(Class ~ ., data = training,kernel = 'polynomial', degree = 2,scale = FALSE);plot(svm_poly, testing)
mean(predict(svm_poly, testing) == testing$Class)
svm_radial <- svm(Class ~ ., data = training,kernel = 'radial',scale = FALSE);plot(svm_radial, testing)
mean(predict(svm_radial, testing) == testing$Class)
##h)-----------------------------------------------------------------------------
svm_poly <- svm(Class ~ ., data = training,kernel = 'polynomial', degree = 2,scale = FALSE);plot(svm_poly, testing)
mean(predict(svm_poly, testing) == testing$Class)
svm_radial <- svm(Class ~ ., data = training,kernel = 'radial',scale = FALSE);plot(svm_radial, testing)
mean(predict(svm_radial, testing) == testing$Class)
###ex8---------------------------------------------------------------------------
##a)-----------------------------------------------------------------------------
require(ISLR); require(tidyverse); require(ggthemes);require(caret); require(e1071)
set.seed(1);data('OJ')
inTrain <- sample(nrow(OJ), 800, replace = FALSE)
training <- OJ[inTrain,];testing <- OJ[-inTrain,]
##b)-----------------------------------------------------------------------------
svm_linear <- svm(Purchase ~ ., data = training,kernel = 'linear',cost = 0.01);summary(svm_linear)
##c)-----------------------------------------------------------------------------
postResample(predict(svm_linear, training), training$Purchase)
##c)-----------------------------------------------------------------------------
postResample(predict(svm_linear, training), training$Purchase)
postResample(predict(svm_linear, testing), testing$Purchase)
##c)-----------------------------------------------------------------------------
mean(predict(svm_linear, training) != training$Class);mean(predict(svm_linear, testing) != testing$Class)
###ex8---------------------------------------------------------------------------
##a)-----------------------------------------------------------------------------
require(ISLR); require(tidyverse); require(ggthemes);require(caret); require(e1071)
set.seed(1);data('OJ')
inTrain <- sample(nrow(OJ), 800, replace = FALSE)
training <- OJ[inTrain,];testing <- OJ[-inTrain,]
##b)-----------------------------------------------------------------------------
svm_linear <- svm(Purchase ~ ., data = training,kernel = 'linear',cost = 0.01);summary(svm_linear)
##c)-----------------------------------------------------------------------------
mean(predict(svm_linear, training) != training$Class);mean(predict(svm_linear, testing) != testing$Class)
##b)-----------------------------------------------------------------------------
svm_linear <- svm(Purchase ~ ., data = training,kernel = 'linear',cost = 0.01,scale=FALSE);summary(svm_linear)
##c)-----------------------------------------------------------------------------
mean(predict(svm_linear, training) != training$Class);mean(predict(svm_linear, testing) != testing$Class)
##b)-----------------------------------------------------------------------------
svm_linear <- svm(Purchase ~ ., data = training,kernel = 'linear',cost = 0.01);summary(svm_linear)
##c)-----------------------------------------------------------------------------
mean(predict(svm_linear, training) != training$Class);mean(predict(svm_linear, testing) != testing$Class)
##c)-----------------------------------------------------------------------------
mean(predict(svm_linear, training) != training$Purchase);mean(predict(svm_linear, testing) != testing$Purchase)
##d)-----------------------------------------------------------------------------
svm.tune <- train(Purchase ~ ., data = training, method = 'svmLinear2', trControl = trainControl(method = 'cv', number = 10),
preProcess = c('center', 'scale'),tuneGrid = expand.grid(cost = seq(0.01, 10, length.out = 20)));svm.tune
##d)-----------------------------------------------------------------------------
svm.tune = tune(svm_linear,Purchase~.,data=OJ.train,ranges=list(cost=c(.01,.02,.05,.1,.2,.5,1,2,5,10)),kernel=linear);summary(svm.tune)
##d)-----------------------------------------------------------------------------
svm.tune = tune(svm_linear,Purchase~.,data=OJ.train,ranges=list(cost=c(.01,.02,.05,.1,.2,.5,1,2,5,10)),kernel='linear');summary(svm.tune)
##d)-----------------------------------------------------------------------------
svm.tune = tune(svm_linear,Purchase~.,data=training,ranges=list(cost=c(.01,.02,.05,.1,.2,.5,1,2,5,10)),kernel='linear');summary(svm.tune)
##d)-----------------------------------------------------------------------------
svm_linear = svm(Purchase~.,data=training,kernel=linear,cost=0.01)
summary(svm1)
##d)-----------------------------------------------------------------------------
svm_linear = svm(Purchase~.,data=training,kernel='linear',cost=0.01)
svm.tune = tune(svm_linear,Purchase~.,data=training,ranges=list(cost=c(.01,.02,.05,.1,.2,.5,1,2,5,10)),kernel='linear');summary(svm.tune)
##d)-----------------------------------------------------------------------------
svm.tune = tune(svm_linear,Purchase~.,data=training,ranges=list(cost=c(.01,.02,.05,.1,.2,.5,1,2,5,10)),kernel='linear');summary(svm.tune)
##d)-----------------------------------------------------------------------------
svm.tune = tune(svm, Purchase ~ ., data = training, kernel = "linear", ranges = list(cost = 10^seq(-2, 1, by = 0.25)));summary(svm.tune)
##b)-----------------------------------------------------------------------------
svm_linear <- svm(Purchase ~ ., data = training,kernel = 'linear',cost = 0.01);summary(svm_linear)
##c)-----------------------------------------------------------------------------
mean(predict(svm_linear, training) != training$Purchase);mean(predict(svm_linear, testing) != testing$Purchase)
##d)-----------------------------------------------------------------------------
svm.tune = tune(svm_linear, Purchase ~ ., data = training, kernel = "linear", ranges = list(cost = 10^seq(-2, 1, by = 0.25)));summary(svm.tune)
##d)-----------------------------------------------------------------------------
svm.tune = tune(svm, Purchase ~ ., data = training, kernel = "linear", ranges = list(cost = 10^seq(-2, 1, by = 0.25)));summary(svm.tune)
##e)-----------------------------------------------------------------------------
svm_linear <- svm(Purchase ~ ., kernel = "linear", data = training, cost = svm.tune$best.parameter$cost)
pred <- predict(svm_linear, training)
##e)-----------------------------------------------------------------------------
svm_linear <- svm(Purchase ~ ., kernel = "linear", data = training, cost = svm.tune$best.parameter$cost)
mean(predict(svm_linear, training) != training$Purchase);mean(predict(svm_linear, testing) != testing$Purchase)
##f)-----------------------------------------------------------------------------
svm_radial <- svm(Purchase ~ ., kernel = "radial", data = training);summary(svm_radial)
mean(predict(svm_radial, training) != training$Purchase);mean(predict(svm_radial, testing) != testing$Purchase)
svm.tune2 = tune(svm, Purchase ~ ., data = training, kernel = "radial", ranges = list(cost = 10^seq(-2, 1, by = 0.25)));summary(svm.tune2)
svm_radial <- svm(Purchase ~ ., kernel = "radial", data = training, cost = svm.tune2$best.parameter$cost)
mean(predict(svm_radial, training) != training$Purchase);mean(predict(svm_radial, testing) != testing$Purchase)
##g)-----------------------------------------------------------------------------
svm_poly <- svm(Purchase ~ ., kernel = "polynomial", data = training,degree=2);summary(svm_poly)
mean(predict(svm_poly, training) != training$Purchase);mean(predict(svm_poly, testing) != testing$Purchase)
svm.tune3 = tune(svm, Purchase ~ ., data = training, kernel = "polynomial", ,degree=2, ranges = list(cost = 10^seq(-2, 1, by = 0.25)));summary(svm.tune3)
svm.tune3 = tune(svm, Purchase ~ ., data = training, kernel = "polynomial", degree=2, ranges = list(cost = 10^seq(-2, 1, by = 0.25)));summary(svm.tune3)
svm_poly <- svm(Purchase ~ ., kernel = "polynomial", data = training, cost = svm.tune3$best.parameter$cost)
mean(predict(svm_poly, training) != training$Purchase);mean(predict(svm_poly, testing) != testing$Purchase)
##g)-----------------------------------------------------------------------------
svm_poly <- svm(Purchase ~ ., kernel = "polynomial", data = training,degree=2);summary(svm_poly)
mean(predict(svm_poly, training) != training$Purchase);mean(predict(svm_poly, testing) != testing$Purchase)
svm.tune3 = tune(svm, Purchase ~ ., data = training, kernel = "polynomial", degree=2, ranges = list(cost = 10^seq(-2, 1, by = 0.25)));summary(svm.tune3)
svm_poly <- svm(Purchase ~ ., kernel = "polynomial", data = training, cost = svm.tune3$best.parameter$cost)
mean(predict(svm_poly, training) != training$Purchase);mean(predict(svm_poly, testing) != testing$Purchase)
