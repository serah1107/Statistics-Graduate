{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from urllib.parse import urlparse, parse_qs, urljoin\n",
    "\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "dir_path = \"C:\\\\Users\\\\user\\\\Desktop\\\\python\\\\datamining\\\\컷툰_작가_시간\\\\\" ## 수정부분\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAVER_URL = \"https://comic.naver.com/index.nhn\"\n",
    "TOP_URL = \"https://comic.naver.com/webtoon/weekday.nhn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(data, file_name):\n",
    "    file = open(file_name, 'a')\n",
    "    file.write(data + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daylywebtoons():\n",
    "    '''\n",
    "    요일 웹툰을 수집\n",
    "    '''\n",
    "\n",
    "    webtoon_main_url = TOP_URL\n",
    "\n",
    "    res = rq.get(webtoon_main_url)\n",
    "\n",
    "    main_soup = BeautifulSoup(res.content, 'lxml')\n",
    "\n",
    "    webtoon_links = [{\"title\": a_tag.get('title'), \"link\": urljoin(NAVER_URL, a_tag.get('href'))}\n",
    "                      for a_tag in main_soup.select('.daily_all a.title')]\n",
    "\n",
    "    return webtoon_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_link(webtoon_url, page_count):\n",
    "    return webtoon_url + '&page=' + str(page_count)\n",
    "\n",
    "\n",
    "def get_qs(url, key):\n",
    "    url_query = urlparse(url).query\n",
    "    query_dict = parse_qs(url_query)\n",
    "    value = query_dict[key][0]\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_all_webtoon(webtoon, is_save):\n",
    "    '''\n",
    "    해당 웹 툰의  1화 ~ 마지막까지 수집\n",
    "    '''\n",
    "    page_count = 1\n",
    "\n",
    "    target_webtoons = list()\n",
    "    webtoon_url = webtoon['link']\n",
    "    webtoon_title = webtoon['title']\n",
    "\n",
    "    webtoon_id = get_qs(webtoon_url, 'titleId')\n",
    "    weekday = get_qs(webtoon_url, 'weekday')\n",
    "\n",
    "    is_unlast = True\n",
    "\n",
    "    while is_unlast:\n",
    "        link = make_link(webtoon_url, page_count)\n",
    "\n",
    "        target_webtoon_res = rq.get(link)\n",
    "        webtoon_soup = BeautifulSoup(target_webtoon_res.content, 'lxml')\n",
    "        a_tags = webtoon_soup.select('.viewList td.title a')\n",
    "\n",
    "        for a_tag in a_tags:\n",
    "            t = a_tag.text.replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "            h = urljoin(NAVER_URL, a_tag.get('href'))\n",
    "\n",
    "            if h not in target_webtoons:\n",
    "                target_webtoons.append(h)\n",
    "            else:\n",
    "                is_unlast = False\n",
    "\n",
    "        page_count += 1\n",
    "\n",
    "    if is_save:\n",
    "        for webtoon in target_webtoons:\n",
    "            save(webtoon_title + ':' + webtoon, 'all_webtoons.txt')\n",
    "\n",
    "    return webtoon_title.strip(), webtoon_id, weekday, target_webtoons\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_parse2(soup, url):\n",
    "    title = soup.title.text.split(':')[0].lstrip(\" \").rstrip(\" \")\n",
    "    no_title = soup.select('.tit_area .view h3')[0].text\n",
    "    no_title = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』姊\\\\‘|\\(\\)\\[\\]\\<\\>`\\'… 【】▲▶◇ⓒⓒ■~★○●◆♥》└ ┘]', '', no_title)\n",
    "    no_title = re.sub('下', '하', no_title)\n",
    "    no_title = re.sub('上', '상', no_title)\n",
    "    no_title = re.sub(\"\\u200b\",\"\",no_title)\n",
    "    no_title = re.sub(\"\\u200b\",\"\",no_title)\n",
    "    \n",
    "    no_date = soup.find(\"dd\",{\"class\":\"date\"}).text\n",
    "    no_time = soup.find(\"input\",{\"name\":\"itemDt\"}).get(\"value\").split(\" \")[1]\n",
    "  \n",
    "    num_cut = soup.find(\"div\",{\"class\":\"wt_viewer\"}).find_all(\"img\",{\"alt\":\"comic content\"})\n",
    "    writer_info = soup.find(\"div\",{\"class\":\"writer_info\"}).find(\"p\").text\n",
    "    \n",
    "    check=0\n",
    "    if len(writer_info) != 1 : \n",
    "        check +=1\n",
    "        \n",
    "    cut_writer = [title,no_title,no_date,no_time,len(num_cut),check]\n",
    " \n",
    "    return(cut_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    webtoons = get_daylywebtoons()\n",
    "    for webtoon in webtoons[61:66]: #index 조정 \n",
    "        webtoon_title, title_id, weekday, target_webtoons = get_all_webtoon(webtoon, False)## 수정부분\n",
    "        os.mkdir(dir_path + webtoon_title)\n",
    "        total =[]\n",
    "        for webtoon_page in target_webtoons:\n",
    "            res = rq.get(webtoon_page)\n",
    "            webtoon_page_soup = BeautifulSoup(res.content, 'lxml')\n",
    "            part = data_parse2(webtoon_page_soup, webtoon_page)\n",
    "            total.append(part)\n",
    "        txttotal = pd.DataFrame(total)\n",
    "        txttotal.to_csv(dir_path+webtoon_title+\"\\\\\"+\"cut_writer\"+'.csv', index=False, header=False,encoding=\"cp949\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
